{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad136f0b",
   "metadata": {},
   "source": [
    "# How good are LLMs at doing probabilistic inference (continuous)\n",
    "\n",
    "The idea is that we pass a bunch of CPTs and we ask the LLM to give us the probability of a specific case.\n",
    "\n",
    "We also ask it to estimate the mean and std.\n",
    "\n",
    "We then compare the LLM result with that of a Bayesian network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd48368",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "from os import getenv\n",
    "\n",
    "from openai import OpenAI\n",
    "from pgmpy.models import LinearGaussianBayesianNetwork\n",
    "\n",
    "# Set the base path\n",
    "base_path = Path(\"../../\")  # One level up from the current working directory\n",
    "\n",
    "# Add the src/ directory to sys.path using base_path\n",
    "sys.path.append(str((base_path / \"src\").resolve()))\n",
    "\n",
    "\n",
    "from bn_utils import draw_bayesian_network\n",
    "from inference_continuous import query_lgbn, format_continuous_query\n",
    "from yaml_utils import load_yaml\n",
    "from llm_calling import run_llm_call"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4825ad8",
   "metadata": {},
   "source": [
    "## Load Bayesian network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dc1020",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cancer_model(use_random=False):\n",
    "    \"\"\"\n",
    "    Create the Linear Gaussian Bayesian Network with either random or specific parameters.\n",
    "    \n",
    "    Args:\n",
    "        use_random (bool): If True, use random parameters. If False, use predefined parameters.\n",
    "    \n",
    "    Returns:\n",
    "        model: LinearGaussianBayesianNetwork with CPDs set\n",
    "        cpds_dict: Dictionary mapping variable names to their CPDs\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create the model structure\n",
    "    model = LinearGaussianBayesianNetwork([\n",
    "        (\"Pollution\", \"Cancer\"),\n",
    "        (\"Smoker\", \"Cancer\"),\n",
    "        (\"Cancer\", \"Xray\"),\n",
    "        (\"Cancer\", \"Dyspnoea\"),\n",
    "    ])\n",
    "    \n",
    "    if use_random:\n",
    "        # Use random parameters\n",
    "        model.get_random_cpds(inplace=True)\n",
    "        print(\"Created model with random parameters\")\n",
    "    else:\n",
    "        # Use specific predefined parameters\n",
    "        from pgmpy.factors.continuous import LinearGaussianCPD\n",
    "        \n",
    "        # P(Pollution) = N(0.305; 1.04)\n",
    "        pollution_cpd = LinearGaussianCPD('Pollution', [0.305], 1.04)\n",
    "        \n",
    "        # P(Smoker) = N(1.446; 0.102)\n",
    "        smoker_cpd = LinearGaussianCPD('Smoker', [1.446], 0.102)\n",
    "        \n",
    "        # P(Cancer | Pollution, Smoker) = N(0.678*Pollution + -0.586*Smoker + 0.244; 0.909)\n",
    "        cancer_cpd = LinearGaussianCPD('Cancer', [0.244, 0.678, -0.586], 0.909, \n",
    "                                      evidence=['Pollution', 'Smoker'])\n",
    "        \n",
    "        # P(Xray | Cancer) = N(-0.623*Cancer + -0.458; 0.135)\n",
    "        xray_cpd = LinearGaussianCPD('Xray', [-0.458, -0.623], 0.135, \n",
    "                                    evidence=['Cancer'])\n",
    "        \n",
    "        # P(Dyspnoea | Cancer) = N(1.218*Cancer + -0.503; 0.271)\n",
    "        dyspnoea_cpd = LinearGaussianCPD('Dyspnoea', [-0.503, 1.218], 0.271, \n",
    "                                        evidence=['Cancer'])\n",
    "        \n",
    "        # Add CPDs to model\n",
    "        model.add_cpds(pollution_cpd, smoker_cpd, cancer_cpd, xray_cpd, dyspnoea_cpd)\n",
    "        print(\"Created model with predefined parameters\")\n",
    "    \n",
    "    # Verify model\n",
    "    assert model.check_model(), \"Model validation failed\"\n",
    "    \n",
    "    # Create CPDs dictionary\n",
    "    cpds_dict = {cpd.variable: cpd for cpd in model.get_cpds()}\n",
    "    \n",
    "    # Display model info\n",
    "    nodes = model.nodes()\n",
    "    edges = model.edges()\n",
    "    cpds = model.get_cpds()\n",
    "    \n",
    "    cpd_strings = []\n",
    "    for cpd in cpds:\n",
    "        cpd_strings.append(str(cpd))\n",
    "    \n",
    "    cpds_as_string = \"\\n\".join(cpd_strings)\n",
    "    \n",
    "    print(f\"\\nNodes in the model: {nodes}\")\n",
    "    print(f\"Edges in the model: {edges}\")\n",
    "    print(f\"CPDs in the model:\")\n",
    "    print(cpds_as_string)\n",
    "    \n",
    "    return model, cpds_dict\n",
    "\n",
    "# Create model with predefined parameters\n",
    "model, cpds_dict = create_cancer_model(use_random=False)\n",
    "\n",
    "# Or create with random parameters:\n",
    "# model, cpds_dict = create_cancer_model(use_random=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16e652f",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_bayesian_network(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e97f2ad",
   "metadata": {},
   "source": [
    "## Prepare prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67f6214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load prompt\n",
    "prompt_path = base_path / \"notebooks\" / \"continuous\" / \"prompts.yaml\"\n",
    "prompts = load_yaml(prompt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28efa87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Posterior estimation query\n",
    "evidence = {'Xray': -1.0, 'Smoker': 2.0}\n",
    "variable = 'Cancer'\n",
    "\n",
    "query_str = format_continuous_query(variable, evidence)\n",
    "\n",
    "prompt_str = prompts[\"prompt_base\"].format(cpts=cpds_as_string, query=query_str)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": prompts[\"system_prompt\"]},\n",
    "    {\"role\": \"user\", \"content\": prompt_str}\n",
    "]\n",
    "\n",
    "print(\"Query:\", query_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20f65db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(prompts[\"system_prompt\"])\n",
    "# print(messages[1][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004cb718",
   "metadata": {},
   "source": [
    "## Run exact inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661b2931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run exact inference for posterior estimation\n",
    "result = query_lgbn(model, variable, evidence)\n",
    "print(f\"Ground truth - Mean: {result['mean']:.4f}, Std: {result['std']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b12b33",
   "metadata": {},
   "source": [
    "## Initialize LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382b0e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL = \"gpt-4o-mini\" # To test the baseline performance of LLMs\n",
    "# MODEL = \"deepseek/deepseek-r1\" \n",
    "# MODEL = \"anthropic/claude-3.7-sonnet:thinking\"\n",
    "# MODEL = \"google/gemini-2.5-pro-preview\"\n",
    "MODEL = \"openai/o3-mini-high\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc23a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI client\n",
    "client = OpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=getenv(\"OPENROUTER_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9491c80d",
   "metadata": {},
   "source": [
    "## Run LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fee7fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "response, messages = run_llm_call(\n",
    "    openai_client=client,\n",
    "    model=MODEL,\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "print(\"LLM Response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347aba66",
   "metadata": {},
   "source": [
    "## Example 2: Probability Calculation Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ebbfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Probability calculation for interval\n",
    "evidence2 = {'Dyspnoea': 0.5}\n",
    "variable2 = 'Pollution'\n",
    "prob_range = (0, 1)  # P(0 < Pollution < 1 | Dyspnoea = 0.5)\n",
    "\n",
    "query_str2 = format_continuous_query(variable2, evidence2, prob_range)\n",
    "\n",
    "prompt_str2 = prompts[\"prompt_base\"].format(cpts=cpds_as_string, query=query_str2)\n",
    "\n",
    "messages2 = [\n",
    "    {\"role\": \"system\", \"content\": prompts[\"system_prompt\"]},\n",
    "    {\"role\": \"user\", \"content\": prompt_str2}\n",
    "]\n",
    "\n",
    "print(\"Query:\", query_str2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4445b123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run exact inference for probability calculation\n",
    "result2 = query_lgbn(model, variable2, evidence2, prob_range)\n",
    "print(f\"Ground truth - Probability: {result2['probability']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33964d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "response2, messages2 = run_llm_call(\n",
    "    openai_client=client,\n",
    "    model=MODEL,\n",
    "    messages=messages2\n",
    ")\n",
    "\n",
    "print(\"LLM Response for probability calculation:\")\n",
    "print(response2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
