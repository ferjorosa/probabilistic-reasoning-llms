{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1be676e",
   "metadata": {},
   "source": [
    "# Objective\n",
    "\n",
    "The purpose of this notebook is to test Pyro with the Asia BN, to get a better understanding of how Pyro works and see if we can use it for our experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ded6404d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import Importance, EmpiricalMarginal\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.infer import MCMC, NUTS\n",
    "from pyro.optim import Adam\n",
    "from pyro import poutine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7820aa",
   "metadata": {},
   "source": [
    "## 1 - Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0ddfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- CPTs wrapped into a helper ---\n",
    "def make_cpts(device):\n",
    "    asia_probs = torch.tensor([0.01, 0.99], device=device)\n",
    "    smoke_probs = torch.tensor([0.5, 0.5], device=device)\n",
    "\n",
    "    tub_probs = torch.tensor([\n",
    "        [0.05, 0.95],\n",
    "        [0.01, 0.99],\n",
    "    ], device=device)\n",
    "\n",
    "    lung_probs = torch.tensor([\n",
    "        [0.1, 0.9],\n",
    "        [0.01, 0.99],\n",
    "    ], device=device)\n",
    "\n",
    "    bronc_probs = torch.tensor([\n",
    "        [0.6, 0.4],\n",
    "        [0.3, 0.7],\n",
    "    ], device=device)\n",
    "\n",
    "    either_probs = torch.tensor([\n",
    "        [[1.0, 0.0], [1.0, 0.0]],  # lung=yes\n",
    "        [[1.0, 0.0], [0.0, 1.0]],  # lung=no\n",
    "    ], device=device)\n",
    "\n",
    "    xray_probs = torch.tensor([\n",
    "        [0.98, 0.02],\n",
    "        [0.05, 0.95],\n",
    "    ], device=device)\n",
    "\n",
    "    dysp_probs = torch.tensor([\n",
    "        [[0.9, 0.1], [0.8, 0.2]],\n",
    "        [[0.7, 0.3], [0.1, 0.9]],\n",
    "    ], device=device)\n",
    "\n",
    "    return asia_probs, smoke_probs, tub_probs, lung_probs, bronc_probs, either_probs, xray_probs, dysp_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ccd24a",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "In order to test the inference, we are going to first test that it works sequentially with Importance Sampling and then we are going to test different algorithms:\n",
    "\n",
    "* Importance sampling\n",
    "* SVI\n",
    "* MCMC (does not work)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d586992",
   "metadata": {},
   "source": [
    "### Sequentially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f15f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model factory (closes over CPTs) ---\n",
    "def make_model(device):\n",
    "    asia_probs, smoke_probs, tub_probs, lung_probs, bronc_probs, either_probs, xray_probs, dysp_probs = make_cpts(device)\n",
    "\n",
    "    def model():\n",
    "        asia = pyro.sample(\"asia\", dist.Categorical(asia_probs))\n",
    "        smoke = pyro.sample(\"smoke\", dist.Categorical(smoke_probs))\n",
    "        tub = pyro.sample(\"tub\", dist.Categorical(tub_probs[asia]))\n",
    "        lung = pyro.sample(\"lung\", dist.Categorical(lung_probs[smoke]))\n",
    "        bronc = pyro.sample(\"bronc\", dist.Categorical(bronc_probs[smoke]))\n",
    "        either = pyro.sample(\"either\", dist.Categorical(either_probs[lung, tub]))\n",
    "        xray = pyro.sample(\"xray\", dist.Categorical(xray_probs[either]))\n",
    "        dysp = pyro.sample(\"dysp\", dist.Categorical(dysp_probs[bronc, either]))\n",
    "        return\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73dac581",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sequential_inference(num_samples=5000, device=\"cpu\", query=\"dysp\", evidence=None):\n",
    "    if evidence is None:\n",
    "        evidence = {}\n",
    "\n",
    "    device = torch.device(device)\n",
    "    pyro.clear_param_store()\n",
    "\n",
    "    # Build model with CPTs on device\n",
    "    model = make_model(device)\n",
    "\n",
    "    # Convert evidence to tensors on the same device\n",
    "    evidence_tensors = {k: torch.tensor(v, device=device) for k, v in evidence.items()}\n",
    "\n",
    "    # Conditioned model\n",
    "    conditioned = poutine.condition(model, data=evidence_tensors)\n",
    "\n",
    "    # Importance sampling\n",
    "    start = time.time()\n",
    "    importance = Importance(conditioned, num_samples=num_samples)\n",
    "    posterior = importance.run()\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "    # Extract samples\n",
    "    marginal = EmpiricalMarginal(posterior, sites=query)\n",
    "    samples = [marginal().item() for _ in range(num_samples)]\n",
    "    samples = torch.tensor(samples)\n",
    "\n",
    "    counts = torch.bincount(samples, minlength=2).float()\n",
    "    probs = counts / counts.sum()\n",
    "\n",
    "    return probs, elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3445fcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Example usage ---\n",
    "# probs_cpu, t_cpu_seq = run_sequential_inference(num_samples=10000, device=\"cpu\", query=\"dysp\", evidence={\"smoke\": 1})\n",
    "\n",
    "# print(f\"CPU: {probs_cpu.numpy()} (time {t_cpu_seq:.3f}s)\")\n",
    "\n",
    "# probs_gpu, t_gpu_seq = run_sequential_inference(num_samples=10000, device=\"cuda\", query=\"dysp\", evidence={\"smoke\": 1})\n",
    "\n",
    "# print(f\"GPU: {probs_gpu.numpy()} (time {t_gpu_seq:.3f}s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02488ddc",
   "metadata": {},
   "source": [
    "### Vectorized (Parallel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Base Model Setup =====\n",
    "def make_vectorized_model(num_samples, device):\n",
    "    \"\"\"Create a vectorized Pyro model using pyro.plate\"\"\"\n",
    "    asia_probs, smoke_probs, tub_probs, lung_probs, bronc_probs, either_probs, xray_probs, dysp_probs = make_cpts(device)\n",
    "    \n",
    "    def vectorized_model():\n",
    "        with pyro.plate(\"particles\", num_samples):\n",
    "            asia = pyro.sample(\"asia\", dist.Categorical(asia_probs))\n",
    "            smoke = pyro.sample(\"smoke\", dist.Categorical(smoke_probs))\n",
    "            tub = pyro.sample(\"tub\", dist.Categorical(tub_probs[asia]))\n",
    "            lung = pyro.sample(\"lung\", dist.Categorical(lung_probs[smoke]))\n",
    "            bronc = pyro.sample(\"bronc\", dist.Categorical(bronc_probs[smoke]))\n",
    "            either = pyro.sample(\"either\", dist.Categorical(either_probs[lung, tub]))\n",
    "            xray = pyro.sample(\"xray\", dist.Categorical(xray_probs[either]))\n",
    "            dysp = pyro.sample(\"dysp\", dist.Categorical(dysp_probs[bronc, either]))\n",
    "            return dysp\n",
    "    \n",
    "    return vectorized_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Importance Sampling =====\n",
    "def run_importance_sampling(num_samples, device, query, evidence):\n",
    "    \"\"\"Vectorized Importance Sampling\"\"\"\n",
    "    \n",
    "    vectorized_model = make_vectorized_model(num_samples, device)\n",
    "    evidence_tensors = {k: torch.tensor(v, device=device).expand(num_samples) for k, v in evidence.items()}\n",
    "    conditioned = poutine.condition(vectorized_model, data=evidence_tensors)\n",
    "    \n",
    "    start = time.time()\n",
    "    importance = Importance(conditioned, num_samples=1)\n",
    "    posterior = importance.run()\n",
    "    traces = list(posterior.exec_traces)\n",
    "    samples = traces[0].nodes[query][\"value\"]\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    return samples, elapsed\n",
    "\n",
    "\n",
    "# ===== Stochastic Variational Inference =====\n",
    "def run_svi_inference(num_samples, device, query, evidence):\n",
    "    \"\"\"Stochastic Variational Inference\"\"\"\n",
    "    \n",
    "    vectorized_model = make_vectorized_model(num_samples, device)\n",
    "    evidence_tensors = {k: torch.tensor(v, device=device).expand(num_samples) for k, v in evidence.items()}\n",
    "    conditioned = poutine.condition(vectorized_model, data=evidence_tensors)\n",
    "    \n",
    "    # Simple mean-field variational family that respects evidence\n",
    "    def guide():\n",
    "        with pyro.plate(\"particles\", num_samples):\n",
    "            # Only sample variables that are NOT observed\n",
    "            if \"asia\" not in evidence:\n",
    "                asia_param = pyro.param(\"asia_param\", torch.ones(2, device=device))\n",
    "                asia = pyro.sample(\"asia\", dist.Categorical(torch.softmax(asia_param, -1)))\n",
    "            \n",
    "            if \"smoke\" not in evidence:\n",
    "                smoke_param = pyro.param(\"smoke_param\", torch.ones(2, device=device))\n",
    "                smoke = pyro.sample(\"smoke\", dist.Categorical(torch.softmax(smoke_param, -1)))\n",
    "            \n",
    "            if \"tub\" not in evidence:\n",
    "                tub_param = pyro.param(\"tub_param\", torch.ones(2, device=device))\n",
    "                tub = pyro.sample(\"tub\", dist.Categorical(torch.softmax(tub_param, -1)))\n",
    "            \n",
    "            if \"lung\" not in evidence:\n",
    "                lung_param = pyro.param(\"lung_param\", torch.ones(2, device=device))\n",
    "                lung = pyro.sample(\"lung\", dist.Categorical(torch.softmax(lung_param, -1)))\n",
    "            \n",
    "            if \"bronc\" not in evidence:\n",
    "                bronc_param = pyro.param(\"bronc_param\", torch.ones(2, device=device))\n",
    "                bronc = pyro.sample(\"bronc\", dist.Categorical(torch.softmax(bronc_param, -1)))\n",
    "            \n",
    "            if \"either\" not in evidence:\n",
    "                either_param = pyro.param(\"either_param\", torch.ones(2, device=device))\n",
    "                either = pyro.sample(\"either\", dist.Categorical(torch.softmax(either_param, -1)))\n",
    "            \n",
    "            if \"xray\" not in evidence:\n",
    "                xray_param = pyro.param(\"xray_param\", torch.ones(2, device=device))\n",
    "                xray = pyro.sample(\"xray\", dist.Categorical(torch.softmax(xray_param, -1)))\n",
    "            \n",
    "            if \"dysp\" not in evidence:\n",
    "                dysp_param = pyro.param(\"dysp_param\", torch.ones(2, device=device))\n",
    "                dysp = pyro.sample(\"dysp\", dist.Categorical(torch.softmax(dysp_param, -1)))\n",
    "    \n",
    "    start = time.time()\n",
    "    # Run SVI optimization\n",
    "    svi = SVI(conditioned, guide, Adam({\"lr\": 0.01}), loss=Trace_ELBO())\n",
    "    \n",
    "    # Optimize for a few steps\n",
    "    for step in range(100):\n",
    "        svi.step()\n",
    "    \n",
    "    # Sample from the learned guide\n",
    "    guide_trace = poutine.trace(guide).get_trace()\n",
    "    samples = guide_trace.nodes[query][\"value\"]\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    return samples, elapsed\n",
    "\n",
    "\n",
    "# ===== MCMC with NUTS =====\n",
    "# def run_mcmc_inference(num_samples, device, query, evidence):\n",
    "#     \"\"\"MCMC with NUTS - Optimized for faster execution\"\"\"\n",
    "    \n",
    "#     vectorized_model = make_vectorized_model(num_samples, device)\n",
    "#     evidence_tensors = {k: torch.tensor(v, device=device).expand(num_samples) for k, v in evidence.items()}\n",
    "#     conditioned = poutine.condition(vectorized_model, data=evidence_tensors)\n",
    "    \n",
    "#     start = time.time()\n",
    "#     # Use fewer samples for faster testing - MCMC is slow for discrete models\n",
    "#     mcmc_samples = min(50, num_samples // 200)  # Much fewer samples\n",
    "#     warmup = min(10, mcmc_samples // 5)  # Short warmup\n",
    "    \n",
    "#     nuts_kernel = NUTS(conditioned, step_size=0.1, adapt_step_size=True, max_tree_depth=3)\n",
    "#     mcmc = MCMC(nuts_kernel, num_samples=mcmc_samples, warmup_steps=warmup, num_chains=2)\n",
    "#     mcmc.run()\n",
    "    \n",
    "#     # Get samples from all chains and repeat to match requested sample size\n",
    "#     samples_dict = mcmc.get_samples()\n",
    "#     mcmc_samples_tensor = samples_dict[query].flatten()  # Flatten all chains\n",
    "    \n",
    "#     # Repeat samples to match requested sample size (approximate)\n",
    "#     repeat_factor = num_samples // len(mcmc_samples_tensor) + 1\n",
    "#     samples = mcmc_samples_tensor.repeat(repeat_factor)[:num_samples]\n",
    "#     elapsed = time.time() - start\n",
    "    \n",
    "#     return samples, elapsed\n",
    "\n",
    "# ===== Unified Interface =====\n",
    "def run_vectorized_inference_multi(num_samples=5000, device=\"cpu\", query=\"dysp\", evidence=None, algorithm=\"importance\"):\n",
    "    \"\"\"\n",
    "    Vectorized inference with multiple algorithm support\n",
    "    \n",
    "    Args:\n",
    "        algorithm: \"importance\", \"svi\", \"mcmc\"\n",
    "    \"\"\"\n",
    "    if evidence is None:\n",
    "        evidence = {}\n",
    "    \n",
    "    device = torch.device(device)\n",
    "    pyro.clear_param_store()\n",
    "    \n",
    "    # Route to appropriate algorithm\n",
    "    if algorithm == \"importance\":\n",
    "        samples, elapsed = run_importance_sampling(num_samples, device, query, evidence)\n",
    "    elif algorithm == \"svi\":\n",
    "        samples, elapsed = run_svi_inference(num_samples, device, query, evidence)\n",
    "    elif algorithm == \"mcmc\":\n",
    "        samples, elapsed = run_mcmc_inference(num_samples, device, query, evidence)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown algorithm: {algorithm}\")\n",
    "    \n",
    "    # Compute probabilities\n",
    "    counts = torch.bincount(samples, minlength=2).float()\n",
    "    probs = counts / counts.sum()\n",
    "    \n",
    "    return probs, elapsed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a66bd703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Vectorized Inference Algorithm Comparison ===\n",
      "\n",
      "=== CPU Results ===\n",
      "Running importance on cpu...\n",
      " IMPORTANCE: [0.32041 0.67959] (time: 0.061s)\n",
      "Running svi on cpu...\n",
      " SVI: [0.69049 0.30951] (time: 3.735s)\n",
      "\n",
      "=== CUDA Results ===\n",
      "Running importance on cuda...\n",
      " IMPORTANCE: [0.31892 0.68108] (time: 0.119s)\n",
      "Running svi on cuda...\n",
      " SVI: [0.68728 0.31272] (time: 0.698s)\n",
      "\n",
      "=== Performance Summary ===\n",
      "IMPORTANCE: GPU 0.51x faster than CPU (0.119s vs 0.061s)\n",
      "SVI: GPU 5.35x faster than CPU (0.698s vs 3.735s)\n"
     ]
    }
   ],
   "source": [
    "# Test multiple vectorized inference algorithms\n",
    "print(\"=== Vectorized Inference Algorithm Comparison ===\\n\")\n",
    "\n",
    "algorithms = [\n",
    "    \"importance\", # really fast\n",
    "    \"svi\", # slow\n",
    "    # \"mcmc\"\n",
    "]\n",
    "\n",
    "devices = [\"cpu\", \"cuda\"]\n",
    "n_samples = 100000\n",
    "results = {}\n",
    "\n",
    "for device in devices:\n",
    "    print(f\"=== {device.upper()} Results ===\")\n",
    "    results[device] = {}\n",
    "\n",
    "    for algorithm in algorithms:\n",
    "        try:\n",
    "            print(f\"Running {algorithm} on {device}...\")\n",
    "            probs, elapsed = run_vectorized_inference_multi(\n",
    "                num_samples=n_samples,\n",
    "                device=device,\n",
    "                query=\"dysp\",\n",
    "                evidence={\"smoke\": 1},\n",
    "                algorithm=algorithm\n",
    "            )\n",
    "\n",
    "            # Handle GPU tensor conversion for display\n",
    "            if device == \"cuda\":\n",
    "                probs_np = probs.cpu().numpy()\n",
    "            else:\n",
    "                probs_np = probs.numpy()\n",
    "\n",
    "            results[device][algorithm] = {\"probs\": probs_np, \"time\": elapsed}\n",
    "            print(f\" {algorithm.upper()}: {probs_np} (time: {elapsed:.3f}s)\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\" {algorithm.upper()}: Failed - {e}\")\n",
    "            results[device][algorithm] = {\"probs\": None, \"time\": None}\n",
    "\n",
    "    print()\n",
    "\n",
    "# Summary comparison\n",
    "print(\"=== Performance Summary ===\")\n",
    "for algorithm in algorithms:\n",
    "    cpu_time = results[\"cpu\"][algorithm][\"time\"]\n",
    "    gpu_time = results[\"cuda\"][algorithm][\"time\"]\n",
    "\n",
    "    if cpu_time is not None and gpu_time is not None:\n",
    "        speedup = cpu_time / gpu_time if gpu_time > 0 else float('inf')\n",
    "        print(f\"{algorithm.upper()}: GPU {speedup:.2f}x faster than CPU ({gpu_time:.3f}s vs {cpu_time:.3f}s)\")\n",
    "    else:\n",
    "        print(f\"{algorithm.upper()}: Could not compare (one failed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c558398",
   "metadata": {},
   "source": [
    "We can see that SVI gives somewhat correct answer but it is flipped. This is an issue of Mean-field Variational inference."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
