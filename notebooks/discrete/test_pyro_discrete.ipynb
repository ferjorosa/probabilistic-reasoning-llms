{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1be676e",
   "metadata": {},
   "source": [
    "# Objective\n",
    "\n",
    "The purpose of this notebook is to test Pyro with the Asia BN, to get a better understanding of how Pyro works and see if we can use it for our experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ded6404d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import Importance, EmpiricalMarginal\n",
    "from pyro import poutine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7820aa",
   "metadata": {},
   "source": [
    "## 1 - Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b0ddfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- CPTs wrapped into a helper ---\n",
    "def make_cpts(device):\n",
    "    asia_probs = torch.tensor([0.01, 0.99], device=device)\n",
    "    smoke_probs = torch.tensor([0.5, 0.5], device=device)\n",
    "\n",
    "    tub_probs = torch.tensor([\n",
    "        [0.05, 0.95],\n",
    "        [0.01, 0.99],\n",
    "    ], device=device)\n",
    "\n",
    "    lung_probs = torch.tensor([\n",
    "        [0.1, 0.9],\n",
    "        [0.01, 0.99],\n",
    "    ], device=device)\n",
    "\n",
    "    bronc_probs = torch.tensor([\n",
    "        [0.6, 0.4],\n",
    "        [0.3, 0.7],\n",
    "    ], device=device)\n",
    "\n",
    "    either_probs = torch.tensor([\n",
    "        [[1.0, 0.0], [1.0, 0.0]],  # lung=yes\n",
    "        [[1.0, 0.0], [0.0, 1.0]],  # lung=no\n",
    "    ], device=device)\n",
    "\n",
    "    xray_probs = torch.tensor([\n",
    "        [0.98, 0.02],\n",
    "        [0.05, 0.95],\n",
    "    ], device=device)\n",
    "\n",
    "    dysp_probs = torch.tensor([\n",
    "        [[0.9, 0.1], [0.8, 0.2]],\n",
    "        [[0.7, 0.3], [0.1, 0.9]],\n",
    "    ], device=device)\n",
    "\n",
    "    return asia_probs, smoke_probs, tub_probs, lung_probs, bronc_probs, either_probs, xray_probs, dysp_probs\n",
    "\n",
    "# --- Model factory (closes over CPTs) ---\n",
    "def make_model(device):\n",
    "    asia_probs, smoke_probs, tub_probs, lung_probs, bronc_probs, either_probs, xray_probs, dysp_probs = make_cpts(device)\n",
    "\n",
    "    def model():\n",
    "        asia = pyro.sample(\"asia\", dist.Categorical(asia_probs))\n",
    "        smoke = pyro.sample(\"smoke\", dist.Categorical(smoke_probs))\n",
    "        tub = pyro.sample(\"tub\", dist.Categorical(tub_probs[asia]))\n",
    "        lung = pyro.sample(\"lung\", dist.Categorical(lung_probs[smoke]))\n",
    "        bronc = pyro.sample(\"bronc\", dist.Categorical(bronc_probs[smoke]))\n",
    "        either = pyro.sample(\"either\", dist.Categorical(either_probs[lung, tub]))\n",
    "        xray = pyro.sample(\"xray\", dist.Categorical(xray_probs[either]))\n",
    "        dysp = pyro.sample(\"dysp\", dist.Categorical(dysp_probs[bronc, either]))\n",
    "        return\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ccd24a",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "In order to test the inference, we are going"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d586992",
   "metadata": {},
   "source": [
    "### Sequentially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73dac581",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sequential_inference(num_samples=5000, device=\"cpu\", query=\"dysp\", evidence=None):\n",
    "    if evidence is None:\n",
    "        evidence = {}\n",
    "\n",
    "    device = torch.device(device)\n",
    "    pyro.clear_param_store()\n",
    "\n",
    "    # Build model with CPTs on device\n",
    "    model = make_model(device)\n",
    "\n",
    "    # Convert evidence to tensors on the same device\n",
    "    evidence_tensors = {k: torch.tensor(v, device=device) for k, v in evidence.items()}\n",
    "\n",
    "    # Conditioned model\n",
    "    conditioned = poutine.condition(model, data=evidence_tensors)\n",
    "\n",
    "    # Importance sampling\n",
    "    start = time.time()\n",
    "    importance = Importance(conditioned, num_samples=num_samples)\n",
    "    posterior = importance.run()\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "    # Extract samples\n",
    "    marginal = EmpiricalMarginal(posterior, sites=query)\n",
    "    samples = [marginal().item() for _ in range(num_samples)]\n",
    "    samples = torch.tensor(samples)\n",
    "\n",
    "    counts = torch.bincount(samples, minlength=2).float()\n",
    "    probs = counts / counts.sum()\n",
    "\n",
    "    return probs, elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3445fcc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU: [0.3276 0.6724] (time 10.543s)\n",
      "GPU: [0.3122 0.6878] (time 30.747s)\n"
     ]
    }
   ],
   "source": [
    "# --- Example usage ---\n",
    "probs_cpu, t_cpu_seq = run_sequential_inference(num_samples=10000, device=\"cpu\", query=\"dysp\", evidence={\"smoke\": 1})\n",
    "\n",
    "print(f\"CPU: {probs_cpu.numpy()} (time {t_cpu_seq:.3f}s)\")\n",
    "\n",
    "probs_gpu, t_gpu_seq = run_sequential_inference(num_samples=10000, device=\"cuda\", query=\"dysp\", evidence={\"smoke\": 1})\n",
    "\n",
    "print(f\"GPU: {probs_gpu.numpy()} (time {t_gpu_seq:.3f}s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02488ddc",
   "metadata": {},
   "source": [
    "### Vectorized (Parallel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_vectorized_inference(num_samples=5000, device=\"cpu\", query=\"dysp\", evidence=None):\n",
    "    \"\"\"Vectorized version using pyro.plate for true GPU parallelization\"\"\"\n",
    "    if evidence is None:\n",
    "        evidence = {}\n",
    "    \n",
    "    device = torch.device(device)\n",
    "    pyro.clear_param_store()\n",
    "    \n",
    "    # Get CPTs\n",
    "    asia_probs, smoke_probs, tub_probs, lung_probs, bronc_probs, either_probs, xray_probs, dysp_probs = make_cpts(device)\n",
    "    \n",
    "    # Vectorized model using pyro.plate\n",
    "    def vectorized_model():\n",
    "        with pyro.plate(\"particles\", num_samples):\n",
    "            asia = pyro.sample(\"asia\", dist.Categorical(asia_probs))\n",
    "            smoke = pyro.sample(\"smoke\", dist.Categorical(smoke_probs))\n",
    "            tub = pyro.sample(\"tub\", dist.Categorical(tub_probs[asia]))\n",
    "            lung = pyro.sample(\"lung\", dist.Categorical(lung_probs[smoke]))\n",
    "            bronc = pyro.sample(\"bronc\", dist.Categorical(bronc_probs[smoke]))\n",
    "            either = pyro.sample(\"either\", dist.Categorical(either_probs[lung, tub]))\n",
    "            xray = pyro.sample(\"xray\", dist.Categorical(xray_probs[either]))\n",
    "            dysp = pyro.sample(\"dysp\", dist.Categorical(dysp_probs[bronc, either]))\n",
    "            return dysp\n",
    "    \n",
    "    # Apply evidence conditioning\n",
    "    evidence_tensors = {k: torch.tensor(v, device=device).expand(num_samples) for k, v in evidence.items()}\n",
    "    conditioned = poutine.condition(vectorized_model, data=evidence_tensors)\n",
    "    \n",
    "    # Run inference (only need 1 sample since each contains num_samples particles)\n",
    "    start = time.time()\n",
    "    importance = Importance(conditioned, num_samples=1)\n",
    "    posterior = importance.run()\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    # Extract results from vectorized sampling\n",
    "    traces = list(posterior.exec_traces)\n",
    "    samples = traces[0].nodes[query][\"value\"]  # All samples in one tensor\n",
    "    \n",
    "    counts = torch.bincount(samples, minlength=2).float()\n",
    "    probs = counts / counts.sum()\n",
    "    \n",
    "    return probs, elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Vectorized Inference Comparison ===\n",
      "CPU (vectorized) time: 2.728s\n",
      "CPU (vectorized) probs: [0.3191492 0.6808508]\n",
      "GPU (vectorized) time: 0.024s\n",
      "GPU (vectorized) probs: [0.3190166 0.6809834]\n",
      "GPU is 114.83x faster than CPU with vectorization!\n"
     ]
    }
   ],
   "source": [
    "# Test vectorized inference - CPU vs GPU comparison\n",
    "print(\"=== Vectorized Inference Comparison ===\")\n",
    "\n",
    "n_samples = 10000000\n",
    "\n",
    "# CPU vectorized\n",
    "probs_cpu_vec, t_cpu_vec = run_vectorized_inference(\n",
    "    num_samples=n_samples, device=\"cpu\", query=\"dysp\", evidence={\"smoke\": 1}\n",
    ")\n",
    "print(f\"CPU (vectorized) time: {t_cpu_vec:.3f}s\")\n",
    "print(f\"CPU (vectorized) probs: {probs_cpu_vec.numpy()}\")\n",
    "\n",
    "# GPU vectorized\n",
    "probs_gpu_vec, t_gpu_vec = run_vectorized_inference(\n",
    "    num_samples=n_samples, device=\"cuda\", query=\"dysp\", evidence={\"smoke\": 1}\n",
    ")\n",
    "print(f\"GPU (vectorized) time: {t_gpu_vec:.3f}s\")\n",
    "print(f\"GPU (vectorized) probs: {probs_gpu_vec.cpu().numpy()}\")\n",
    "\n",
    "# GPU vs CPU comparison\n",
    "gpu_speedup = t_cpu_vec / t_gpu_vec\n",
    "print(f\"GPU is {gpu_speedup:.2f}x faster than CPU with vectorization!\")\n",
    "\n",
    "# Compare sequential GPU vs vectorized GPU (only if we use the same number of samples)\n",
    "# print(f\"Improvement over sequential GPU: {t_gpu_seq/t_gpu_vec:.2f}x faster\\n\")\n",
    "# print(f\"Improvement over sequential CPU: {t_cpu_seq/t_cpu_vec:.2f}x faster\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
