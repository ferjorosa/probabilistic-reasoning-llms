{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM calls\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import re\n",
    "from os import getenv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "\n",
    "repo_root = Path(\".\").resolve().parents[1]\n",
    "sys.path.append(str(repo_root / 'src'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_generation import generate_dag_with_treewidth\n",
    "from bn_generation import generate_variants_for_dag\n",
    "from cpd_utils import cpd_to_ascii_table\n",
    "from discrete.discrete_inference import format_probability_query, query_probability\n",
    "from llm_calling import extract_numeric_answer, create_probability_prompt, run_llm_call \n",
    "from yaml_utils import load_yaml\n",
    "from bn_query_sweep import compute_query_complexity, compute_all_query_complexities, generate_bayesian_networks_and_metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM Configuration and Helper Functions\n",
    "# GLOBAL TOGGLE: Set to False to disable all LLM calls\n",
    "ENABLE_LLM_CALLS = False\n",
    "\n",
    "MODEL = \"openai/gpt-5\"\n",
    "MODEL = \"deepseek/deepseek-chat-v3.1:free\"\n",
    "MODEL = \"openai/o3-mini-high\"  \n",
    "\n",
    "# Initialize OpenAI client (only if LLM calls are enabled)\n",
    "if ENABLE_LLM_CALLS:\n",
    "    client = OpenAI(\n",
    "        base_url=\"https://openrouter.ai/api/v1\",\n",
    "        api_key=getenv(\"OPENROUTER_API_KEY\")\n",
    "    )\n",
    "else:\n",
    "    client = None\n",
    "    print(\"LLM calls are DISABLED. Set ENABLE_LLM_CALLS = True to enable.\")\n",
    "\n",
    "# Load prompts\n",
    "prompt_path = repo_root / \"notebooks\" / \"discrete\" / \"prompts.yaml\"\n",
    "prompts = load_yaml(prompt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Call LLM on a subset AFTER query generation\n",
    "# Provide indices of rows in full_df for which to call the LLM.\n",
    "# Import _parse_field from bn_query_sweep instead of defining it locally\n",
    "from bn_query_sweep import _parse_field, call_llm_for_query\n",
    "\n",
    "# Ensure LLM columns exist\n",
    "if 'llm_probability' not in full_df.columns:\n",
    "    full_df['llm_probability'] = None\n",
    "if 'llm_response' not in full_df.columns:\n",
    "    full_df['llm_response'] = None\n",
    "\n",
    "# Select which rows to send to LLM (example below commented out)\n",
    "#selected_indices = list(full_df.sample(n=40, random_state=0).index)\n",
    "selected_indices = list(full_df.sample(n=5, random_state=0).index)\n",
    "#selected_indices = list(full_df.index)\n",
    "\n",
    "for ridx in selected_indices:\n",
    "    row = full_df.iloc[ridx]\n",
    "    bn = all_bayesian_networks[int(row['bn_index'])]['bn']\n",
    "    query_vars = _parse_field(row['query_vars']) or []\n",
    "    query_states = _parse_field(row['query_states']) or []\n",
    "    evidence = _parse_field(row['evidence']) or None\n",
    "    print(f\"Processing BN {int(row['bn_index'])}/{len(all_bayesian_networks)}, Query {int(row['query_index'])}...\")\n",
    "    llm_prob, llm_response = call_llm_for_query(bn, query_vars, query_states, evidence)\n",
    "    full_df.at[ridx, 'llm_probability'] = llm_prob\n",
    "    full_df.at[ridx, 'llm_response'] = llm_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "# Add the model as a column. If not present, add and set to used model (from call_llm_for_query)\n",
    "used_model = MODEL\n",
    "if 'model' not in full_df.columns:\n",
    "    full_df['model'] = used_model\n",
    "else:\n",
    "    full_df['model'].fillna(used_model, inplace=True)\n",
    "\n",
    "# Drop BN and query identifier columns before saving (if present)\n",
    "cols_to_drop = [col for col in ['bn_index', 'query_index'] if col in full_df.columns]\n",
    "full_df_no_ids = full_df.drop(columns=cols_to_drop) if cols_to_drop else full_df\n",
    "\n",
    "# Instead of relying on out_query_csv, construct the path directly\n",
    "out_llm_csv = Path(f\"llm_query_results_{timestamp}_with_llm.csv\")\n",
    "full_df_no_ids.to_csv(out_llm_csv, index=False)\n",
    "print(\"Saved DataFrame with LLM responses to\", out_llm_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print rows with non-empty llm_probability\n",
    "llm_rows = full_df[full_df['llm_probability'].notna()]\n",
    "print(f\"Found {len(llm_rows)} rows with LLM probability values:\")\n",
    "print(\"=\" * 80)\n",
    "display(llm_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display LLM performance statistics\n",
    "print(\"LLM Performance Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Count successful LLM responses\n",
    "successful_llm = full_df['llm_probability'].notna().sum()\n",
    "total_queries = len(full_df)\n",
    "print(f\"Successful LLM responses: {successful_llm}/{total_queries} ({successful_llm/total_queries*100:.1f}%)\")\n",
    "\n",
    "# Save enhanced results with LLM data\n",
    "enhanced_csv = repo_root / 'notebooks' / 'graph_generation' / 'bn_generation_sweep_queries_with_llm.csv'\n",
    "full_df.to_csv(enhanced_csv, index=False)\n",
    "print(f'Saved enhanced results with LLM data to {enhanced_csv}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy metrics for successful responses\n",
    "if successful_llm > 0:\n",
    "    # Filter to only successful LLM responses\n",
    "    successful_df = full_df[full_df['llm_probability'].notna() & full_df['probability'].notna()]\n",
    "    \n",
    "    if len(successful_df) > 0:\n",
    "        # Calculate absolute errors\n",
    "        successful_df = successful_df.copy()\n",
    "        successful_df['abs_error'] = abs(successful_df['llm_probability'] - successful_df['probability'])\n",
    "        successful_df['rel_error'] = successful_df['abs_error'] / successful_df['probability']\n",
    "        \n",
    "        print(f\"\\nAccuracy Metrics (for {len(successful_df)} successful responses):\")\n",
    "        print(f\"Mean Absolute Error: {successful_df['abs_error'].mean():.6f}\")\n",
    "        print(f\"Mean Relative Error: {successful_df['rel_error'].mean():.6f}\")\n",
    "        print(f\"Max Absolute Error: {successful_df['abs_error'].max():.6f}\")\n",
    "        print(f\"Max Relative Error: {successful_df['rel_error'].max():.6f}\")\n",
    "        \n",
    "        # Show some examples\n",
    "        print(f\"\\nFirst 5 successful responses:\")\n",
    "        display(successful_df[['query_vars', 'query_states', 'evidence', 'probability', 'llm_probability', 'abs_error']].head())\n",
    "    else:\n",
    "        print(\"No successful LLM responses with exact inference results to compare.\")\n",
    "else:\n",
    "    print(\"No successful LLM responses.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Set up the plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Filter to only successful LLM responses with exact inference results\n",
    "plot_df = full_df[full_df['llm_probability'].notna() & full_df['probability'].notna()].copy()\n",
    "plot_df['abs_error'] = abs(plot_df['llm_probability'] - plot_df['probability'])\n",
    "\n",
    "if len(plot_df) > 0:\n",
    "    # Identify all BN and query property columns (exclude result columns)\n",
    "    exclude_cols = {'bn_index', 'query_vars', 'query_states', 'evidence', 'probability', \n",
    "                   'llm_probability', 'llm_response', 'abs_error', 'rel_error', 'target_tw', 'n', 'seed', 'variant_index',\n",
    "                   'alpha', 'determinism', 'arity', 'query_index', \n",
    "                   'achieved_tw', 'num_nodes'}\n",
    "    \n",
    "    # Get all columns that are BN or query properties\n",
    "    property_cols = [col for col in full_df.columns if col not in exclude_cols]\n",
    "    \n",
    "    # Calculate number of subplots needed\n",
    "    n_props = len(property_cols)\n",
    "    n_cols = min(4, n_props)  # Max 4 columns\n",
    "    n_rows = (n_props + n_cols - 1) // n_cols  # Ceiling division\n",
    "    \n",
    "    # Create figure with subplots for all properties\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(4*n_cols, 3*n_rows))\n",
    "    fig.suptitle('Absolute Error by BN and Query Properties', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Flatten axes for easier indexing\n",
    "    if n_props == 1:\n",
    "        axes = [axes]\n",
    "    elif n_rows == 1:\n",
    "        axes = axes.flatten()\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    # Create boxplots for each property\n",
    "    for i, prop in enumerate(property_cols):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Get unique values for this property\n",
    "        unique_vals = sorted(plot_df[prop].unique())\n",
    "        \n",
    "        # Create boxplot data\n",
    "        box_data = []\n",
    "        labels = []\n",
    "        \n",
    "        for val in unique_vals:\n",
    "            subset = plot_df[plot_df[prop] == val]['abs_error']\n",
    "            if len(subset) > 0:  # Only include if there's data\n",
    "                box_data.append(subset.values)\n",
    "                labels.append(str(val))\n",
    "        \n",
    "        if box_data:  # Only plot if we have data\n",
    "            ax.boxplot(box_data, labels=labels)\n",
    "            ax.set_title(f'Absolute Error by {prop.replace(\"_\", \" \").title()}')\n",
    "            ax.set_xlabel(prop.replace(\"_\", \" \").title())\n",
    "            ax.set_ylabel('Absolute Error')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Rotate x-axis labels if there are many unique values\n",
    "            if len(labels) > 5:\n",
    "                ax.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(n_props, len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics for each property\n",
    "    print(\"\\nSummary Statistics by Property:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for prop in property_cols:\n",
    "        print(f\"\\n{prop.upper()}:\")\n",
    "        prop_stats = plot_df.groupby(prop)['abs_error'].agg(['count', 'mean', 'std', 'min', 'max'])\n",
    "        print(prop_stats.round(6))\n",
    "        \n",
    "else:\n",
    "    print(\"No successful LLM responses with exact inference results available for plotting.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute average distance of each number from 0 to 10 to all others\n",
    "\n",
    "numbers = list(range(11))  # 0 to 10\n",
    "dists = []\n",
    "for x in numbers:\n",
    "    distances = [abs(x - y)**2 for y in numbers]\n",
    "    avg_distance = sum(distances) / len(distances)\n",
    "    print(f\"{x}: {avg_distance:.2f}\")\n",
    "    dists.append(avg_distance)\n",
    "\n",
    "from math import sqrt\n",
    "sqrt(sum(dists) / len(dists))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
