{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BN Generation Parameter Sweep\n",
    "\n",
    "This notebook sweeps over DAG/BN generation parameters outlined in `notebooks/graph_generation/ideas.md` and materializes multiple discrete BN variants per DAG.\n",
    "\n",
    "It varies:\n",
    "- n (number of variables)\n",
    "- target treewidth\n",
    "- variable arity (fixed or range)\n",
    "- CPT skewness (Dirichlet alpha)\n",
    "- determinism fraction (mostly 0%)\n",
    "\n",
    "Outputs:\n",
    "- CSV with per-variant metadata\n",
    "- On-screen CPT previews for a small sample\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import re\n",
    "from os import getenv\n",
    "\n",
    "# Ensure src is importable\n",
    "repo_root = Path(\".\").resolve().parents[1]\n",
    "sys.path.append(str(repo_root / 'src'))\n",
    "\n",
    "from graph_generation import generate_dag_with_treewidth\n",
    "from bn_generation import generate_variants_for_dag\n",
    "from cpd_utils import cpd_to_ascii_table\n",
    "from discrete.discrete_inference import format_probability_query, query_probability\n",
    "from llm_calling import run_llm_call\n",
    "from yaml_utils import load_yaml\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "\n",
    "# Import query complexity functions and BN generation function from bn_query_sweep module\n",
    "from bn_query_sweep import compute_query_complexity, compute_all_query_complexities, generate_bayesian_networks_and_metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM Configuration and Helper Functions\n",
    "# GLOBAL TOGGLE: Set to False to disable all LLM calls\n",
    "ENABLE_LLM_CALLS = False\n",
    "\n",
    "MODEL = \"deepseek/deepseek-chat-v3.1:free\"\n",
    "MODEL = \"openai/gpt-5\"\n",
    "MODEL = \"openai/o3-mini-high\"  \n",
    "\n",
    "# Initialize OpenAI client (only if LLM calls are enabled)\n",
    "if ENABLE_LLM_CALLS:\n",
    "    client = OpenAI(\n",
    "        base_url=\"https://openrouter.ai/api/v1\",\n",
    "        api_key=getenv(\"OPENROUTER_API_KEY\")\n",
    "    )\n",
    "else:\n",
    "    client = None\n",
    "    print(\"LLM calls are DISABLED. Set ENABLE_LLM_CALLS = True to enable.\")\n",
    "\n",
    "# Load prompts\n",
    "prompt_path = repo_root / \"notebooks\" / \"discrete\" / \"prompts.yaml\"\n",
    "prompts = load_yaml(prompt_path)\n",
    "# Import the functions from llm_calling instead of defining them here\n",
    "from llm_calling import extract_numeric_answer, create_probability_prompt, run_llm_call \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter grids (edit as needed)\n",
    "#ns = [7, 11, 15]\n",
    "#ns = [25]\n",
    "ns = [9, 11]\n",
    "#treewidths = [2, 3, 4]\n",
    "#treewidths = [5]\n",
    "treewidths = [3, 5, 7]\n",
    "arity_specs = [\n",
    "    #{\"type\": \"fixed\", \"fixed\": 2},\n",
    "    {\"type\": \"range\", \"min\": 2, \"max\": 3},\n",
    "]\n",
    "#dirichlet_alphas = [0.5, 1.0]\n",
    "dirichlet_alphas = [1.0, 0.5]\n",
    "#determinism_fracs = [0.0, 0.1]  # mostly 0%; includes a nonzero test\n",
    "determinism_fracs = [0.0]  # mostly 0%; includes a nonzero test\n",
    "#naming_strategies = ['simple', 'confusing', 'semantic']  # Add naming strategy variation\n",
    "#naming_strategies = ['simple', 'confusing']  # Add naming strategy variation\n",
    "naming_strategies = ['confusing']  # Add naming strategy variation\n",
    "variants_per_combo = 4\n",
    "base_seed = 42\n",
    "\n",
    "rows = []\n",
    "preview_samples = []\n",
    "\n",
    "sample_counter = 0\n",
    "all_bayesian_networks = []  # Store all BNs and their metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Bayesian networks and populate metadata using the centralized function\n",
    "all_bayesian_networks, rows, preview_samples = generate_bayesian_networks_and_metadata(\n",
    "    ns=ns,\n",
    "    treewidths=treewidths,\n",
    "    arity_specs=arity_specs,\n",
    "    dirichlet_alphas=dirichlet_alphas,\n",
    "    determinism_fracs=determinism_fracs,\n",
    "    naming_strategies=naming_strategies,\n",
    "    variants_per_combo=variants_per_combo,\n",
    "    base_seed=base_seed,\n",
    "    max_preview_samples=3\n",
    ")\n",
    "\n",
    "print(f\"Generated {len(all_bayesian_networks)} Bayesian networks\")\n",
    "print(f\"Created {len(rows)} metadata rows\")\n",
    "print(f\"Collected {len(preview_samples)} preview samples\")\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "display(df.head())\n",
    "print(f\"Total variants: {len(df)}\")\n",
    "out_csv = repo_root / 'notebooks' / 'graph_generation' / 'bn_generation_sweep.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out BNs where the number of edges is less than twice the number of nodes\n",
    "def num_edges(bn):\n",
    "    # For pgmpy BayesianModel, the edges can be accessed by .edges\n",
    "    return len(list(bn.edges()))\n",
    "\n",
    "filtered_bn_list = []\n",
    "filtered_rows = []\n",
    "\n",
    "# Map original BN indices to their indices in the filtered list\n",
    "original_bn_idx_to_filtered_idx = {}\n",
    "\n",
    "for orig_idx, (bn_dict, row) in enumerate(zip(all_bayesian_networks, rows)):\n",
    "    bn = bn_dict[\"bn\"]\n",
    "    n_nodes = len(bn.nodes())\n",
    "    n_edges = num_edges(bn)\n",
    "    if n_edges >= 2 * n_nodes:\n",
    "        filtered_bn_list.append(bn_dict)\n",
    "        filtered_rows.append(row)\n",
    "        original_bn_idx_to_filtered_idx[orig_idx] = len(filtered_bn_list) - 1\n",
    "\n",
    "\n",
    "# Overwrite the master lists with the filtered ones\n",
    "all_bayesian_networks = filtered_bn_list\n",
    "rows = filtered_rows\n",
    "\n",
    "print(f\"After filtering, {len(all_bayesian_networks)} Bayesian networks remain.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(rows)\n",
    "display(df.head())\n",
    "print(f\"Total variants: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each Bayesian network, generate n queries using generate_queries, run them, and collect results\n",
    "from query_generation import generate_queries\n",
    "from pgmpy.inference import VariableElimination\n",
    "\n",
    "# Store all queries for later recovery: a list of lists (per BN)\n",
    "all_bn_queries = []\n",
    "\n",
    "query_rows = []\n",
    "\n",
    "def count_unobserved_ancestors(bn, target_nodes, evidence_nodes):\n",
    "    \"\"\"\n",
    "    For the given Bayesian network, count the number of unique ancestors of the\n",
    "    target_nodes that are not in evidence_nodes.\n",
    "    \"\"\"\n",
    "    all_ancestors = set()\n",
    "    for v in target_nodes:\n",
    "        # Use the public method from pgmpy's BayesianNetwork to get ancestors\n",
    "        # Recursively collect ancestors using get_parents\n",
    "        def get_ancestors(node, bn, visited=None):\n",
    "            if visited is None:\n",
    "                visited = set()\n",
    "            parents = set(bn.get_parents(node))\n",
    "            new_parents = parents - visited\n",
    "            visited.update(new_parents)\n",
    "            for p in new_parents:\n",
    "                get_ancestors(p, bn, visited)\n",
    "            return visited\n",
    "\n",
    "        all_ancestors.update(get_ancestors(v, bn))\n",
    "    return len(all_ancestors - set(evidence_nodes))\n",
    "\n",
    "for idx, bn_dict in enumerate(all_bayesian_networks):\n",
    "    bn = bn_dict[\"bn\"]\n",
    "    # Use a different seed per BN for query generation for reproducibility\n",
    "    query_seed = 1000 + idx\n",
    "    # Generate queries for this BN\n",
    "    queries = generate_queries(\n",
    "        bn,\n",
    "        num_queries=24,\n",
    "        query_node_counts=(1, 2),\n",
    "        #query_node_counts=[2],\n",
    "        evidence_counts=(0, 1, 2),\n",
    "        #evidence_counts=(2),\n",
    "        #distance_buckets=[(1, 1), (2, 3), (1, 3)],\n",
    "        distance_buckets=[(2, 3)],\n",
    "        seed=query_seed,\n",
    "    )\n",
    "    all_bn_queries.append(queries)\n",
    "    # Get the BN's properties from the main df\n",
    "    bn_row = df.iloc[idx].to_dict()\n",
    "    for qidx, query in enumerate(queries):\n",
    "        # Prepare inference\n",
    "        infer = VariableElimination(bn)\n",
    "        # Query variables and their states\n",
    "        query_vars = [v for v, _ in query.targets]\n",
    "        query_states = [s for _, s in query.targets]\n",
    "        # Evidence dict: variable -> state\n",
    "        evidence = query.evidence if query.evidence else None\n",
    "        evidence_nodes = list(query.evidence.keys()) if query.evidence else []\n",
    "\n",
    "        # Count number of ancestors of target nodes that are not observed (not in evidence)\n",
    "        num_unobserved_ancestors = count_unobserved_ancestors(bn, query_vars, evidence_nodes)\n",
    "\n",
    "        # Compute exact probability (with evidence)\n",
    "        try:\n",
    "            result = infer.query(variables=query_vars, evidence=evidence, show_progress=False)\n",
    "            assignment = dict(zip(query_vars, query_states))\n",
    "            prob = result.get_value(**assignment)\n",
    "        except Exception as e:\n",
    "            prob = None\n",
    "\n",
    "        # Compute prior probability (no evidence)\n",
    "        try:\n",
    "            prior_result = infer.query(variables=query_vars, evidence=None, show_progress=False)\n",
    "            prior_assignment = dict(zip(query_vars, query_states))\n",
    "            prior_prob = prior_result.get_value(**prior_assignment)\n",
    "        except Exception as e:\n",
    "            prior_prob = None\n",
    "\n",
    "        # Collect all info for the table, merging BN and query properties (no LLM here)\n",
    "        row = dict(bn_row)  # copy BN properties\n",
    "        row.update({\n",
    "            \"bn_index\": idx,\n",
    "            \"query_index\": qidx,\n",
    "            \"query_vars\": str(query_vars),\n",
    "            \"query_states\": str(query_states),\n",
    "            \"evidence\": str(query.evidence),\n",
    "            \"distance\": query.meta.get(\"min_target_evidence_distance\"),\n",
    "            \"num_evidence\": query.meta.get(\"num_evidence_nodes\"),\n",
    "            \"probability\": prob,\n",
    "            \"prior_probability\": prior_prob,  # add the prior probability as a new column\n",
    "            \"num_unobserved_ancestors\": num_unobserved_ancestors,  # new column per instructions\n",
    "        })\n",
    "        query_rows.append(row)\n",
    "\n",
    "# Convert to DataFrame and display\n",
    "full_df = pd.DataFrame(query_rows)\n",
    "print(f\"Total queries: {len(full_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "prior = full_df[\"prior_probability\"].astype(float)\n",
    "posterior = full_df[\"probability\"].astype(float)\n",
    "\n",
    "# Mask out rows where prior or posterior is NaN\n",
    "mask = ~(prior.isna() | posterior.isna())\n",
    "prior = prior[mask]\n",
    "posterior = posterior[mask]\n",
    "\n",
    "abs_diff = np.abs(posterior - prior)\n",
    "# Avoid division by zero; mask entries where prior=0 for rel_diff\n",
    "with np.errstate(divide='ignore', invalid='ignore'):\n",
    "    rel_diff = np.where(prior != 0, (posterior - prior) / prior, np.nan)\n",
    "\n",
    "fig, axs = plt.subplots(3, 1, figsize=(8, 10), sharex=True)\n",
    "\n",
    "# Absolute difference plot\n",
    "axs[0].scatter(prior, abs_diff, alpha=0.6)\n",
    "axs[0].set_ylabel(\"Absolute Difference |Posterior - Prior|\")\n",
    "axs[0].set_title(\"Absolute Difference vs. Prior Probability\")\n",
    "axs[0].grid(True)\n",
    "\n",
    "# Relative difference plot\n",
    "axs[1].scatter(prior, rel_diff, alpha=0.6)\n",
    "axs[1].set_xlabel(\"Prior Probability\")\n",
    "axs[1].set_ylabel(\"Relative Difference (Posterior - Prior) / Prior\")\n",
    "axs[1].set_title(\"Relative Difference vs. Prior Probability\")\n",
    "axs[1].grid(True)\n",
    "\n",
    "\n",
    "axs[2].scatter(abs_diff, rel_diff, alpha=0.6)\n",
    "axs[2].set_xlabel(\"abs diff\")\n",
    "axs[2].set_ylabel(\"Relative Difference (Posterior - Prior) / Prior\")\n",
    "axs[2].set_title(\"Relative Difference vs. Abs Diff\")\n",
    "axs[2].grid(True)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot the number of unobserved ancestors for cases with no evidence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# \"No evidence\" -> evidence set is empty; so filter rows with len(evidence)==0\n",
    "mask_no_evidence = full_df['num_evidence'] == 0\n",
    "\n",
    "no_evidence_df = full_df[mask_no_evidence]\n",
    "\n",
    "unobs_anc_counts = no_evidence_df[\"num_unobserved_ancestors\"]\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.hist(unobs_anc_counts, bins=range(int(unobs_anc_counts.min()), int(unobs_anc_counts.max())+2), edgecolor=\"black\", alpha=0.7)\n",
    "plt.title(\"Distribution of # Unobserved Ancestors (No Evidence)\")\n",
    "plt.xlabel(\"Number of Unobserved Ancestors\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.grid(axis=\"y\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Keep only queries with at least 3 unobserved ancestors\n",
    "full_df = full_df[full_df[\"num_unobserved_ancestors\"] >= 3].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a row index from full_df to analyze\n",
    "test_row_index = 0  # Change this to any valid row index\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"TESTING QUERY-SPECIFIC COMPLEXITY COMPUTATION FOR ROW {test_row_index}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Compute complexity for the selected row\n",
    "complexity_result = compute_query_complexity(full_df, all_bayesian_networks, test_row_index, verbose=True)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DETAILED COMPLEXITY METRICS:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Display the results in a nice format\n",
    "for key, value in complexity_result.items():\n",
    "    if key not in ['elimination_order', 'complete_elimination_order', 'factor_sizes', 'query_vars', 'query_states', 'evidence', 'keep_vars', 'eliminate_vars']:\n",
    "        print(f\"{key:25}: {value}\")\n",
    "    elif key in ['elimination_order', 'complete_elimination_order']:\n",
    "        print(f\"{key:25}: {value[:5]}... (showing first 5 of {len(value)})\")\n",
    "    elif key == 'factor_sizes':\n",
    "        print(f\"{key:25}: {value[:5]}... (showing first 5 of {len(value)})\")\n",
    "    else:\n",
    "        print(f\"{key:25}: {value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FACTOR SIZE PROGRESSION:\")\n",
    "print(\"=\" * 80)\n",
    "for i, size in enumerate(complexity_result['factor_sizes']):\n",
    "    print(f\"Step {i+1:2d}: {size:8,} entries\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"QUERY-SPECIFIC ANALYSIS:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Query variables kept: {complexity_result['keep_vars']}\")\n",
    "print(f\"Variables eliminated: {complexity_result['eliminate_vars']}\")\n",
    "print(f\"Variables eliminated: {complexity_result['num_eliminated_vars']}/{complexity_result['num_vars']} ({complexity_result['num_eliminated_vars']/complexity_result['num_vars']*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "complexity_df = compute_all_query_complexities(full_df, all_bayesian_networks, verbose=True)\n",
    "print(\"\\nComplexity DataFrame:\")\n",
    "display(complexity_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of query complexities and their relation to Bayesian network treewidth\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Workaround: Add achieved_tw to complexity_df by joining with rows metadata if missing\n",
    "if 'achieved_tw' not in complexity_df.columns:\n",
    "    # Try to import the metadata rows (used in sweep code)\n",
    "    # This block expects all_bayesian_networks and their 'meta'\n",
    "    try:\n",
    "        # Find which DataFrame in our namespace has 'achieved_tw'\n",
    "        # Typically \"rows\" or maybe \"full_df\" has this info\n",
    "        if 'rows' in globals():\n",
    "            meta_df = pd.DataFrame(rows)\n",
    "        else:\n",
    "            # Try to get the meta attributes via all_bayesian_networks\n",
    "            meta_df = pd.DataFrame([bn['meta'] for bn in all_bayesian_networks])\n",
    "        # Try to align row indices between complexity_df and meta_df; use 'bn_index' if available\n",
    "        if 'bn_index' in complexity_df.columns:\n",
    "            meta_df = meta_df.reset_index().rename(columns={'index': 'bn_index'})\n",
    "            merged_df = complexity_df.merge(meta_df[['bn_index', 'achieved_tw']], on='bn_index', how='left')\n",
    "        elif 'row_index' in complexity_df.columns:\n",
    "            meta_df = meta_df.reset_index().rename(columns={'index': 'row_index'})\n",
    "            merged_df = complexity_df.merge(meta_df[['row_index', 'achieved_tw']], on='row_index', how='left')\n",
    "        else:\n",
    "            # fallback: just concat if lengths match\n",
    "            if len(meta_df) == len(complexity_df):\n",
    "                merged_df = complexity_df.copy()\n",
    "                merged_df['achieved_tw'] = meta_df['achieved_tw']\n",
    "            else:\n",
    "                print(\"Warning: Could not align metadata with complexity_df. 'achieved_tw' will be missing.\")\n",
    "                merged_df = complexity_df\n",
    "        complexity_df = merged_df\n",
    "    except Exception as e:\n",
    "        print(\"Could not add 'achieved_tw' to complexity_df:\", e)\n",
    "\n",
    "if 'achieved_tw' not in complexity_df.columns:\n",
    "    print(\"Error: 'achieved_tw' column is missing from complexity_df and could not be merged. Summary cannot proceed.\")\n",
    "    summary_df = complexity_df.dropna(subset=['induced_width'])  # Only dropna on what we do have\n",
    "else:\n",
    "    summary_df = complexity_df.dropna(subset=['induced_width', 'achieved_tw'])\n",
    "\n",
    "print(\"Summary of Query Complexities vs. Bayesian Network Treewidth\")\n",
    "print(\"-\" * 70)\n",
    "print(\"Number of queries analyzed:\", len(summary_df))\n",
    "if 'error' in summary_df.columns:\n",
    "    print(\"Number of failed computations:\", summary_df['error'].notna().sum())\n",
    "\n",
    "if 'achieved_tw' in summary_df.columns:\n",
    "    # Compute means grouped by achieved treewidth\n",
    "    grouped = summary_df.groupby('achieved_tw').agg(\n",
    "        mean_induced_width=('induced_width', 'mean'),\n",
    "        std_induced_width=('induced_width', 'std'),\n",
    "        mean_total_cost=('total_cost', 'mean'),\n",
    "        mean_max_factor_size=('max_factor_size', 'mean'),\n",
    "        num_queries=('induced_width', 'count'),\n",
    "    )\n",
    "    print(\"\\nAverage query-specific induced width, total cost, and factor sizes by BN treewidth:\")\n",
    "    display(grouped)\n",
    "    \n",
    "    # Compare induced width to achieved treewidth\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.scatter(summary_df['achieved_tw'], summary_df['induced_width'], alpha=0.7)\n",
    "    plt.plot(sorted(summary_df['achieved_tw'].unique()), sorted(summary_df['achieved_tw'].unique()), 'r--', label='BN treewidth')\n",
    "    plt.xlabel(\"Achieved BN Treewidth\")\n",
    "    plt.ylabel(\"Query-Specific Induced Width\")\n",
    "    plt.title(\"Induced Width for Queries vs. BN Treewidth\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.scatter(summary_df['achieved_tw'], summary_df['total_cost'], alpha=0.7)\n",
    "    plt.xlabel(\"Achieved BN Treewidth\")\n",
    "    plt.ylabel(\"Query Elimination Total Cost\")\n",
    "    plt.title(\"Total Cost vs. Achieved BN Treewidth (Query-Specific)\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Skipping treewidth summary/grouped plots because 'achieved_tw' is unavailable in this DataFrame.\")\n",
    "\n",
    "# Textual summary\n",
    "print(\"\\nTextual Insights:\")\n",
    "print(\n",
    "    \" - Query-specific induced width is typically less than or equal to the overall BN treewidth (diagonal shown in red).\\n\"\n",
    "    \" - Variability arises due to evidence and targeted query variables, leading to factors smaller than the worst-case elimination.\\n\"\n",
    "    \" - Some queries are much easier than others on the same BN; induced width and computational cost are query- and evidence-dependent.\\n\"\n",
    "    \" - Cost and maximum factor sizes generally increase as BN treewidth increases, but not all queries are equally hard.\\n\"\n",
    "    \" - Review the scatter plots and group means above for your instances.\\n\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect a single row: draw BN, call LLM, compare\\n\n",
    "from bn_query_sweep import inspect_row_and_call_llm, call_llm_for_query\n",
    "from pathlib import Path\n",
    "\n",
    "if ENABLE_LLM_CALLS:\n",
    "    # Choose a row index from full_df\n",
    "    #row_index = 330\n",
    "    row_index = 8\n",
    "\n",
    "    result = inspect_row_and_call_llm(\n",
    "        full_df=full_df,\n",
    "        all_bayesian_networks=all_bayesian_networks,\n",
    "        row_index=row_index,\n",
    "        openai_client=client,\n",
    "        model=MODEL,\n",
    "        prompts_path=prompt_path,\n",
    "        draw_kwargs={\"figsize\": (6, 4)},\n",
    "    )\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Call LLM on a subset AFTER query generation\n",
    "# Provide indices of rows in full_df for which to call the LLM.\n",
    "# Import _parse_field from bn_query_sweep instead of defining it locally\n",
    "from bn_query_sweep import _parse_field\n",
    "\n",
    "# Ensure LLM columns exist\n",
    "if 'llm_probability' not in full_df.columns:\n",
    "    full_df['llm_probability'] = None\n",
    "if 'llm_response' not in full_df.columns:\n",
    "    full_df['llm_response'] = None\n",
    "\n",
    "ENABLE_LLM_CALLS = True\n",
    "if ENABLE_LLM_CALLS:\n",
    "    # Select which rows to send to LLM (example below commented out)\n",
    "    #selected_indices = list(full_df.sample(n=40, random_state=0).index)\n",
    "    selected_indices = list(full_df.sample(n=2, random_state=0).index)\n",
    "    #selected_indices = list(full_df.index)\n",
    "\n",
    "    for ridx in selected_indices:\n",
    "        row = full_df.iloc[ridx]\n",
    "        bn = all_bayesian_networks[int(row['bn_index'])]['bn']\n",
    "        query_vars = _parse_field(row['query_vars']) or []\n",
    "        query_states = _parse_field(row['query_states']) or []\n",
    "        evidence = _parse_field(row['evidence']) or None\n",
    "        print(f\"Processing BN {int(row['bn_index'])}/{len(all_bayesian_networks)}, Query {int(row['query_index'])}...\")\n",
    "        llm_prob, llm_response = call_llm_for_query(bn, query_vars, query_states, evidence)\n",
    "        full_df.at[ridx, 'llm_probability'] = llm_prob\n",
    "        full_df.at[ridx, 'llm_response'] = llm_response\n",
    "else:\n",
    "    print(\"LLM calls are disabled. Skipping batch LLM processing.\")\n",
    "    print(f\"Would have processed {len(full_df)} queries if LLM calls were enabled.\")\n",
    "\n",
    "ENABLE_LLM_CALLS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_llm_csv = out_query_csv.with_name(out_query_csv.stem + \"_with_llm.csv\")\n",
    "# Save the DataFrame with LLM responses to a CSV file\n",
    "#out_llm_csv = out_query_csv.replace(\".csv\", \"_with_llm.csv\")\n",
    "full_df.to_csv(out_llm_csv, index=False)\n",
    "print(\"Saved DataFrame with LLM responses to\", out_llm_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print rows with non-empty llm_probability\n",
    "llm_rows = full_df[full_df['llm_probability'].notna()]\n",
    "print(f\"Found {len(llm_rows)} rows with LLM probability values:\")\n",
    "print(\"=\" * 80)\n",
    "display(llm_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display LLM performance statistics\n",
    "print(\"LLM Performance Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Count successful LLM responses\n",
    "successful_llm = full_df['llm_probability'].notna().sum()\n",
    "total_queries = len(full_df)\n",
    "print(f\"Successful LLM responses: {successful_llm}/{total_queries} ({successful_llm/total_queries*100:.1f}%)\")\n",
    "\n",
    "# Save enhanced results with LLM data\n",
    "enhanced_csv = repo_root / 'notebooks' / 'graph_generation' / 'bn_generation_sweep_queries_with_llm.csv'\n",
    "full_df.to_csv(enhanced_csv, index=False)\n",
    "print(f'Saved enhanced results with LLM data to {enhanced_csv}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy metrics for successful responses\n",
    "if successful_llm > 0:\n",
    "    # Filter to only successful LLM responses\n",
    "    successful_df = full_df[full_df['llm_probability'].notna() & full_df['probability'].notna()]\n",
    "    \n",
    "    if len(successful_df) > 0:\n",
    "        # Calculate absolute errors\n",
    "        successful_df = successful_df.copy()\n",
    "        successful_df['abs_error'] = abs(successful_df['llm_probability'] - successful_df['probability'])\n",
    "        successful_df['rel_error'] = successful_df['abs_error'] / successful_df['probability']\n",
    "        \n",
    "        print(f\"\\nAccuracy Metrics (for {len(successful_df)} successful responses):\")\n",
    "        print(f\"Mean Absolute Error: {successful_df['abs_error'].mean():.6f}\")\n",
    "        print(f\"Mean Relative Error: {successful_df['rel_error'].mean():.6f}\")\n",
    "        print(f\"Max Absolute Error: {successful_df['abs_error'].max():.6f}\")\n",
    "        print(f\"Max Relative Error: {successful_df['rel_error'].max():.6f}\")\n",
    "        \n",
    "        # Show some examples\n",
    "        print(f\"\\nFirst 5 successful responses:\")\n",
    "        display(successful_df[['query_vars', 'query_states', 'evidence', 'probability', 'llm_probability', 'abs_error']].head())\n",
    "    else:\n",
    "        print(\"No successful LLM responses with exact inference results to compare.\")\n",
    "else:\n",
    "    print(\"No successful LLM responses.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Set up the plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Filter to only successful LLM responses with exact inference results\n",
    "plot_df = full_df[full_df['llm_probability'].notna() & full_df['probability'].notna()].copy()\n",
    "plot_df['abs_error'] = abs(plot_df['llm_probability'] - plot_df['probability'])\n",
    "\n",
    "if len(plot_df) > 0:\n",
    "    # Identify all BN and query property columns (exclude result columns)\n",
    "    exclude_cols = {'bn_index', 'query_vars', 'query_states', 'evidence', 'probability', \n",
    "                   'llm_probability', 'llm_response', 'abs_error', 'rel_error', 'target_tw', 'n', 'seed', 'variant_index',\n",
    "                   'alpha', 'determinism', 'arity', 'query_index', \n",
    "                   'achieved_tw', 'num_nodes'}\n",
    "    \n",
    "    # Get all columns that are BN or query properties\n",
    "    property_cols = [col for col in full_df.columns if col not in exclude_cols]\n",
    "    \n",
    "    # Calculate number of subplots needed\n",
    "    n_props = len(property_cols)\n",
    "    n_cols = min(4, n_props)  # Max 4 columns\n",
    "    n_rows = (n_props + n_cols - 1) // n_cols  # Ceiling division\n",
    "    \n",
    "    # Create figure with subplots for all properties\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(4*n_cols, 3*n_rows))\n",
    "    fig.suptitle('Absolute Error by BN and Query Properties', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Flatten axes for easier indexing\n",
    "    if n_props == 1:\n",
    "        axes = [axes]\n",
    "    elif n_rows == 1:\n",
    "        axes = axes.flatten()\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    # Create boxplots for each property\n",
    "    for i, prop in enumerate(property_cols):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Get unique values for this property\n",
    "        unique_vals = sorted(plot_df[prop].unique())\n",
    "        \n",
    "        # Create boxplot data\n",
    "        box_data = []\n",
    "        labels = []\n",
    "        \n",
    "        for val in unique_vals:\n",
    "            subset = plot_df[plot_df[prop] == val]['abs_error']\n",
    "            if len(subset) > 0:  # Only include if there's data\n",
    "                box_data.append(subset.values)\n",
    "                labels.append(str(val))\n",
    "        \n",
    "        if box_data:  # Only plot if we have data\n",
    "            ax.boxplot(box_data, labels=labels)\n",
    "            ax.set_title(f'Absolute Error by {prop.replace(\"_\", \" \").title()}')\n",
    "            ax.set_xlabel(prop.replace(\"_\", \" \").title())\n",
    "            ax.set_ylabel('Absolute Error')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Rotate x-axis labels if there are many unique values\n",
    "            if len(labels) > 5:\n",
    "                ax.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(n_props, len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics for each property\n",
    "    print(\"\\nSummary Statistics by Property:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for prop in property_cols:\n",
    "        print(f\"\\n{prop.upper()}:\")\n",
    "        prop_stats = plot_df.groupby(prop)['abs_error'].agg(['count', 'mean', 'std', 'min', 'max'])\n",
    "        print(prop_stats.round(6))\n",
    "        \n",
    "else:\n",
    "    print(\"No successful LLM responses with exact inference results available for plotting.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_all_query_complexities(full_df, all_bayesian_networks, verbose=False):\n",
    "    \"\"\"\n",
    "    Compute complexity metrics for all queries in full_df.\n",
    "    \n",
    "    Parameters:\n",
    "    - full_df: DataFrame containing query information\n",
    "    - all_bayesian_networks: List of BN dictionaries with 'bn' and 'meta' keys\n",
    "    - verbose: If True, print progress information\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame with complexity metrics for each query\n",
    "    \"\"\"\n",
    "    complexity_results = []\n",
    "    \n",
    "    for idx in range(len(full_df)):\n",
    "        if verbose:\n",
    "            print(f\"Processing query {idx+1}/{len(full_df)}...\")\n",
    "        \n",
    "        try:\n",
    "            result = compute_query_complexity(full_df, all_bayesian_networks, idx, verbose=False)\n",
    "            complexity_results.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing row {idx}: {e}\")\n",
    "            # Add a row with error information\n",
    "            complexity_results.append({\n",
    "                'row_index': idx,\n",
    "                'error': str(e),\n",
    "                'induced_width': None,\n",
    "                'total_cost': None,\n",
    "                'max_factor_size': None,\n",
    "            })\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    complexity_df = pd.DataFrame(complexity_results)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nComputed complexity for {len(complexity_results)} queries\")\n",
    "        if 'error' in complexity_df.columns:\n",
    "            successful = len(complexity_df[complexity_df['error'].isna()])\n",
    "            failed = len(complexity_df[complexity_df['error'].notna()])\n",
    "        else:\n",
    "            successful = len(complexity_df)\n",
    "            failed = 0\n",
    "        print(f\"Successful computations: {successful}\")\n",
    "        print(f\"Failed computations: {failed}\")\n",
    "    \n",
    "    return complexity_df\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
