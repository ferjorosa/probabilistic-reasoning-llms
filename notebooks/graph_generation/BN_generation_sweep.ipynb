{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BN Generation Parameter Sweep\n",
    "\n",
    "This notebook sweeps over DAG/BN generation parameters outlined in `notebooks/graph_generation/ideas.md` and materializes multiple discrete BN variants per DAG.\n",
    "\n",
    "It varies:\n",
    "- n (number of variables)\n",
    "- target treewidth\n",
    "- variable arity (fixed or range)\n",
    "- CPT skewness (Dirichlet alpha)\n",
    "- determinism fraction (mostly 0%)\n",
    "\n",
    "Outputs:\n",
    "- CSV with per-variant metadata\n",
    "- On-screen CPT previews for a small sample\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import re\n",
    "from os import getenv\n",
    "\n",
    "# Ensure src is importable\n",
    "repo_root = Path(\".\").resolve().parents[1]\n",
    "sys.path.append(str(repo_root / 'src'))\n",
    "\n",
    "from graph_generation import generate_dag_with_treewidth\n",
    "from bn_generation import generate_variants_for_dag\n",
    "from cpd_utils import cpd_to_ascii_table\n",
    "from discrete.discrete_inference import format_probability_query, query_probability\n",
    "from llm_calling import run_llm_call\n",
    "from yaml_utils import load_yaml\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM calls are DISABLED. Set ENABLE_LLM_CALLS = True to enable.\n"
     ]
    }
   ],
   "source": [
    "# LLM Configuration and Helper Functions\n",
    "# GLOBAL TOGGLE: Set to False to disable all LLM calls\n",
    "ENABLE_LLM_CALLS = False\n",
    "\n",
    "MODEL = \"deepseek/deepseek-chat-v3.1:free\"\n",
    "MODEL = \"openai/gpt-5\"\n",
    "MODEL = \"openai/o3-mini-high\"  \n",
    "\n",
    "# Initialize OpenAI client (only if LLM calls are enabled)\n",
    "if ENABLE_LLM_CALLS:\n",
    "    client = OpenAI(\n",
    "        base_url=\"https://openrouter.ai/api/v1\",\n",
    "        api_key=getenv(\"OPENROUTER_API_KEY\")\n",
    "    )\n",
    "else:\n",
    "    client = None\n",
    "    print(\"LLM calls are DISABLED. Set ENABLE_LLM_CALLS = True to enable.\")\n",
    "\n",
    "# Load prompts\n",
    "prompt_path = repo_root / \"notebooks\" / \"discrete\" / \"prompts.yaml\"\n",
    "prompts = load_yaml(prompt_path)\n",
    "# Import the functions from llm_calling instead of defining them here\n",
    "from llm_calling import extract_numeric_answer, create_probability_prompt, run_llm_call \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter grids (edit as needed)\n",
    "#ns = [7, 11, 15]\n",
    "#ns = [25]\n",
    "ns = [9]\n",
    "#treewidths = [2, 3, 4]\n",
    "#treewidths = [5]\n",
    "treewidths = [3]\n",
    "arity_specs = [\n",
    "    #{\"type\": \"fixed\", \"fixed\": 2},\n",
    "    {\"type\": \"range\", \"min\": 2, \"max\": 3},\n",
    "]\n",
    "#dirichlet_alphas = [0.5, 1.0]\n",
    "dirichlet_alphas = [1.0]\n",
    "#determinism_fracs = [0.0, 0.1]  # mostly 0%; includes a nonzero test\n",
    "determinism_fracs = [0.0]  # mostly 0%; includes a nonzero test\n",
    "#naming_strategies = ['simple', 'confusing', 'semantic']  # Add naming strategy variation\n",
    "#naming_strategies = ['simple', 'confusing']  # Add naming strategy variation\n",
    "naming_strategies = ['confusing']  # Add naming strategy variation\n",
    "variants_per_combo = 4\n",
    "base_seed = 42\n",
    "\n",
    "rows = []\n",
    "preview_samples = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arity_to_str(spec):\n",
    "    if spec[\"type\"] == \"fixed\":\n",
    "        return f\"fixed:{spec['fixed']}\"\n",
    "    return f\"range:{spec['min']}-{spec['max']}\"\n",
    "\n",
    "sample_counter = 0\n",
    "all_bayesian_networks = []  # Store all BNs and their metadata\n",
    "\n",
    "for n in ns:\n",
    "    for tw in treewidths:\n",
    "        for naming in naming_strategies:\n",
    "            dag, achieved_tw, _ = generate_dag_with_treewidth(n, tw, node_naming=naming, seed=base_seed + sample_counter)\n",
    "            for arity in arity_specs:\n",
    "                for alpha in dirichlet_alphas:\n",
    "                    for det in determinism_fracs:\n",
    "                        cfgs = []\n",
    "                        for i in range(variants_per_combo):\n",
    "                            cfgs.append({\n",
    "                                \"arity_strategy\": arity,\n",
    "                                \"dirichlet_alpha\": alpha,\n",
    "                                \"determinism_fraction\": det,\n",
    "                            })\n",
    "                        variants = generate_variants_for_dag(dag, cfgs, base_seed=base_seed + sample_counter)\n",
    "                        for idx, (bn, meta) in enumerate(variants):\n",
    "                            # Store BN and its metadata for later access\n",
    "                            all_bayesian_networks.append({\n",
    "                                \"bn\": bn,\n",
    "                                \"meta\": {\n",
    "                                    \"n\": n,\n",
    "                                    \"target_tw\": tw,\n",
    "                                    \"achieved_tw\": achieved_tw,\n",
    "                                    \"naming\": naming,\n",
    "                                    \"arity\": arity_to_str(arity),\n",
    "                                    \"alpha\": meta[\"dirichlet_alpha\"],\n",
    "                                    \"determinism\": meta[\"determinism_fraction\"],\n",
    "                                    \"seed\": meta[\"seed\"],\n",
    "                                    \"variant_index\": idx,\n",
    "                                    \"num_edges\": bn.number_of_edges(),\n",
    "                                    \"num_nodes\": bn.number_of_nodes(),\n",
    "                                }\n",
    "                            })\n",
    "                            rows.append({\n",
    "                                \"n\": n,\n",
    "                                \"target_tw\": tw,\n",
    "                                \"achieved_tw\": achieved_tw,\n",
    "                                \"naming\": naming,\n",
    "                                \"arity\": arity_to_str(arity),\n",
    "                                \"alpha\": meta[\"dirichlet_alpha\"],\n",
    "                                \"determinism\": meta[\"determinism_fraction\"],\n",
    "                                \"seed\": meta[\"seed\"],\n",
    "                                \"variant_index\": idx,\n",
    "                                \"num_edges\": bn.number_of_edges(),\n",
    "                                \"num_nodes\": bn.number_of_nodes(),\n",
    "                            })\n",
    "                            if sample_counter < 3:  # collect a few previews\n",
    "                                preview_samples.append(bn)\n",
    "                        sample_counter += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>target_tw</th>\n",
       "      <th>achieved_tw</th>\n",
       "      <th>naming</th>\n",
       "      <th>arity</th>\n",
       "      <th>alpha</th>\n",
       "      <th>determinism</th>\n",
       "      <th>seed</th>\n",
       "      <th>variant_index</th>\n",
       "      <th>num_edges</th>\n",
       "      <th>num_nodes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>confusing</td>\n",
       "      <td>range:2-3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>confusing</td>\n",
       "      <td>range:2-3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10015</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>confusing</td>\n",
       "      <td>range:2-3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19988</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>confusing</td>\n",
       "      <td>range:2-3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29961</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n  target_tw  achieved_tw     naming      arity  alpha  determinism   seed  \\\n",
       "0  9          3            3  confusing  range:2-3    1.0          0.0     42   \n",
       "1  9          3            3  confusing  range:2-3    1.0          0.0  10015   \n",
       "2  9          3            3  confusing  range:2-3    1.0          0.0  19988   \n",
       "3  9          3            3  confusing  range:2-3    1.0          0.0  29961   \n",
       "\n",
       "   variant_index  num_edges  num_nodes  \n",
       "0              0         12          9  \n",
       "1              1         12          9  \n",
       "2              2         12          9  \n",
       "3              3         12          9  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total variants: 4\n",
      "Saved to /home/bmihaljevic/repos/code-projects/aily/probabilistic-reasoning-llms/notebooks/graph_generation/bn_generation_sweep.csv\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(rows)\n",
    "display(df.head())\n",
    "print(f\"Total variants: {len(df)}\")\n",
    "\n",
    "# Save CSV next to notebook\n",
    "out_csv = repo_root / 'notebooks' / 'graph_generation' / 'bn_generation_sweep.csv'\n",
    "df.to_csv(out_csv, index=False)\n",
    "print('Saved to', out_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total queries: 48\n"
     ]
    }
   ],
   "source": [
    "# For each Bayesian network, generate n queries using generate_queries, run them, and collect results\n",
    "from query_generation import generate_queries\n",
    "from pgmpy.inference import VariableElimination\n",
    "\n",
    "# Store all queries for later recovery: a list of lists (per BN)\n",
    "all_bn_queries = []\n",
    "\n",
    "query_rows = []\n",
    "\n",
    "for idx, bn_dict in enumerate(all_bayesian_networks):\n",
    "    bn = bn_dict[\"bn\"]\n",
    "    # Use a different seed per BN for query generation for reproducibility\n",
    "    query_seed = 1000 + idx\n",
    "    # Generate 5 queries for this BN\n",
    "    queries = generate_queries(\n",
    "        bn,\n",
    "        num_queries=12,\n",
    "        query_node_counts=(1, 2),\n",
    "        #query_node_counts=[2],\n",
    "        evidence_counts=(0, 1, 2),\n",
    "        #evidence_counts=(2),\n",
    "        #distance_buckets=[(1, 1), (2, 3), (1, 3)],\n",
    "        distance_buckets=[(2, 3)],\n",
    "        seed=query_seed,\n",
    "    )\n",
    "    all_bn_queries.append(queries)\n",
    "    # Get the BN's properties from the main df\n",
    "    bn_row = df.iloc[idx].to_dict()\n",
    "    for qidx, query in enumerate(queries):\n",
    "        # Prepare inference\n",
    "        infer = VariableElimination(bn)\n",
    "        # Query variables and their states\n",
    "        query_vars = [v for v, _ in query.targets]\n",
    "        query_states = [s for _, s in query.targets]\n",
    "        # Evidence dict: variable -> state\n",
    "        evidence = query.evidence if query.evidence else None\n",
    "\n",
    "        # Compute exact probability\n",
    "        try:\n",
    "            # pgmpy: query returns a factor, we need to index into the right assignment\n",
    "            result = infer.query(variables=query_vars, evidence=evidence, show_progress=False)\n",
    "            # result is a DiscreteFactor, get the probability for the assignment\n",
    "            # The order of query_vars matches the order of query_states\n",
    "            assignment = dict(zip(query_vars, query_states))\n",
    "            prob = result.get_value(**assignment)\n",
    "        except Exception as e:\n",
    "            prob = None\n",
    "\n",
    "        # Collect all info for the table, merging BN and query properties (no LLM here)\n",
    "        row = dict(bn_row)  # copy BN properties\n",
    "        row.update({\n",
    "            \"bn_index\": idx,\n",
    "            \"query_index\": qidx,\n",
    "            \"query_vars\": str(query_vars),\n",
    "            \"query_states\": str(query_states),\n",
    "            \"evidence\": str(query.evidence),\n",
    "            \"distance\": query.meta.get(\"min_target_evidence_distance\"),\n",
    "            \"num_evidence\": query.meta.get(\"num_evidence_nodes\"),\n",
    "            \"probability\": prob,\n",
    "        })\n",
    "        query_rows.append(row)\n",
    "\n",
    "# Convert to DataFrame and display\n",
    "full_df = pd.DataFrame(query_rows)\n",
    "print(f\"Total queries: {len(full_df)}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved query+BN results to /home/bmihaljevic/repos/code-projects/aily/probabilistic-reasoning-llms/results/discrete/bn_queries_openai_o3-mini-high_20250930_125714.csv\n"
     ]
    }
   ],
   "source": [
    "# Optionally, save the full query+BN dataframe\n",
    "#out_query_csv = repo_root / 'notebooks' / 'graph_generation' / 'bn_generation_sweep_queries.csv'\n",
    "from datetime import datetime\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_filename = MODEL.replace(\"/\", \"_\")\n",
    "out_query_csv = repo_root / 'results' / 'discrete' / f'bn_queries_{model_filename}_{timestamp}.csv'\n",
    "# Ensure the directory exists\n",
    "out_query_csv.parent.mkdir(parents=True, exist_ok=True)\n",
    "full_df.to_csv(out_query_csv, index=False)\n",
    "print('Saved query+BN results to', out_query_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect a single row: draw BN, call LLM, compare\\n\n",
    "from bn_query_sweep import inspect_row_and_call_llm, call_llm_for_query\n",
    "from pathlib import Path\n",
    "\n",
    "if ENABLE_LLM_CALLS:\n",
    "    # Choose a row index from full_df\n",
    "    #row_index = 330\n",
    "    row_index = 8\n",
    "\n",
    "    result = inspect_row_and_call_llm(\n",
    "        full_df=full_df,\n",
    "        all_bayesian_networks=all_bayesian_networks,\n",
    "        row_index=row_index,\n",
    "        openai_client=client,\n",
    "        model=MODEL,\n",
    "        prompts_path=prompt_path,\n",
    "        draw_kwargs={\"figsize\": (6, 4)},\n",
    "    )\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM calls are disabled. Skipping batch LLM processing.\n",
      "Would have processed 48 queries if LLM calls were enabled.\n"
     ]
    }
   ],
   "source": [
    "# Optional: Call LLM on a subset AFTER query generation\n",
    "# Provide indices of rows in full_df for which to call the LLM.\n",
    "# Import _parse_field from bn_query_sweep instead of defining it locally\n",
    "from bn_query_sweep import _parse_field\n",
    "\n",
    "# Ensure LLM columns exist\n",
    "if 'llm_probability' not in full_df.columns:\n",
    "    full_df['llm_probability'] = None\n",
    "if 'llm_response' not in full_df.columns:\n",
    "    full_df['llm_response'] = None\n",
    "\n",
    "if ENABLE_LLM_CALLS:\n",
    "    # Select which rows to send to LLM (example below commented out)\n",
    "    #selected_indices = list(full_df.sample(n=40, random_state=0).index)\n",
    "    selected_indices = list(full_df.index)\n",
    "\n",
    "    for ridx in selected_indices:\n",
    "        row = full_df.iloc[ridx]\n",
    "        bn = all_bayesian_networks[int(row['bn_index'])]['bn']\n",
    "        query_vars = _parse_field(row['query_vars']) or []\n",
    "        query_states = _parse_field(row['query_states']) or []\n",
    "        evidence = _parse_field(row['evidence']) or None\n",
    "        print(f\"Processing BN {int(row['bn_index'])}/{len(all_bayesian_networks)}, Query {int(row['query_index'])}...\")\n",
    "        llm_prob, llm_response = call_llm_for_query(bn, query_vars, query_states, evidence)\n",
    "        full_df.at[ridx, 'llm_probability'] = llm_prob\n",
    "        full_df.at[ridx, 'llm_response'] = llm_response\n",
    "else:\n",
    "    print(\"LLM calls are disabled. Skipping batch LLM processing.\")\n",
    "    print(f\"Would have processed {len(full_df)} queries if LLM calls were enabled.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved DataFrame with LLM responses to /home/bmihaljevic/repos/code-projects/aily/probabilistic-reasoning-llms/results/discrete/bn_queries_openai_o3-mini-high_20250930_125714_with_llm.csv\n"
     ]
    }
   ],
   "source": [
    "out_llm_csv = out_query_csv.with_name(out_query_csv.stem + \"_with_llm.csv\")\n",
    "# Save the DataFrame with LLM responses to a CSV file\n",
    "#out_llm_csv = out_query_csv.replace(\".csv\", \"_with_llm.csv\")\n",
    "full_df.to_csv(out_llm_csv, index=False)\n",
    "print(\"Saved DataFrame with LLM responses to\", out_llm_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 rows with LLM probability values:\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>target_tw</th>\n",
       "      <th>achieved_tw</th>\n",
       "      <th>naming</th>\n",
       "      <th>arity</th>\n",
       "      <th>alpha</th>\n",
       "      <th>determinism</th>\n",
       "      <th>seed</th>\n",
       "      <th>variant_index</th>\n",
       "      <th>num_edges</th>\n",
       "      <th>...</th>\n",
       "      <th>bn_index</th>\n",
       "      <th>query_index</th>\n",
       "      <th>query_vars</th>\n",
       "      <th>query_states</th>\n",
       "      <th>evidence</th>\n",
       "      <th>distance</th>\n",
       "      <th>num_evidence</th>\n",
       "      <th>probability</th>\n",
       "      <th>llm_probability</th>\n",
       "      <th>llm_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [n, target_tw, achieved_tw, naming, arity, alpha, determinism, seed, variant_index, num_edges, num_nodes, bn_index, query_index, query_vars, query_states, evidence, distance, num_evidence, probability, llm_probability, llm_response]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 21 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print rows with non-empty llm_probability\n",
    "llm_rows = full_df[full_df['llm_probability'].notna()]\n",
    "print(f\"Found {len(llm_rows)} rows with LLM probability values:\")\n",
    "print(\"=\" * 80)\n",
    "display(llm_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Performance Analysis:\n",
      "==================================================\n",
      "Successful LLM responses: 0/48 (0.0%)\n",
      "Saved enhanced results with LLM data to /home/bmihaljevic/repos/code-projects/aily/probabilistic-reasoning-llms/notebooks/graph_generation/bn_generation_sweep_queries_with_llm.csv\n"
     ]
    }
   ],
   "source": [
    "# Display LLM performance statistics\n",
    "print(\"LLM Performance Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Count successful LLM responses\n",
    "successful_llm = full_df['llm_probability'].notna().sum()\n",
    "total_queries = len(full_df)\n",
    "print(f\"Successful LLM responses: {successful_llm}/{total_queries} ({successful_llm/total_queries*100:.1f}%)\")\n",
    "\n",
    "# Save enhanced results with LLM data\n",
    "enhanced_csv = repo_root / 'notebooks' / 'graph_generation' / 'bn_generation_sweep_queries_with_llm.csv'\n",
    "full_df.to_csv(enhanced_csv, index=False)\n",
    "print(f'Saved enhanced results with LLM data to {enhanced_csv}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No successful LLM responses.\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy metrics for successful responses\n",
    "if successful_llm > 0:\n",
    "    # Filter to only successful LLM responses\n",
    "    successful_df = full_df[full_df['llm_probability'].notna() & full_df['probability'].notna()]\n",
    "    \n",
    "    if len(successful_df) > 0:\n",
    "        # Calculate absolute errors\n",
    "        successful_df = successful_df.copy()\n",
    "        successful_df['abs_error'] = abs(successful_df['llm_probability'] - successful_df['probability'])\n",
    "        successful_df['rel_error'] = successful_df['abs_error'] / successful_df['probability']\n",
    "        \n",
    "        print(f\"\\nAccuracy Metrics (for {len(successful_df)} successful responses):\")\n",
    "        print(f\"Mean Absolute Error: {successful_df['abs_error'].mean():.6f}\")\n",
    "        print(f\"Mean Relative Error: {successful_df['rel_error'].mean():.6f}\")\n",
    "        print(f\"Max Absolute Error: {successful_df['abs_error'].max():.6f}\")\n",
    "        print(f\"Max Relative Error: {successful_df['rel_error'].max():.6f}\")\n",
    "        \n",
    "        # Show some examples\n",
    "        print(f\"\\nFirst 5 successful responses:\")\n",
    "        display(successful_df[['query_vars', 'query_states', 'evidence', 'probability', 'llm_probability', 'abs_error']].head())\n",
    "    else:\n",
    "        print(\"No successful LLM responses with exact inference results to compare.\")\n",
    "else:\n",
    "    print(\"No successful LLM responses.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No successful LLM responses with exact inference results available for plotting.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Set up the plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Filter to only successful LLM responses with exact inference results\n",
    "plot_df = full_df[full_df['llm_probability'].notna() & full_df['probability'].notna()].copy()\n",
    "plot_df['abs_error'] = abs(plot_df['llm_probability'] - plot_df['probability'])\n",
    "\n",
    "if len(plot_df) > 0:\n",
    "    # Identify all BN and query property columns (exclude result columns)\n",
    "    exclude_cols = {'bn_index', 'query_vars', 'query_states', 'evidence', 'probability', \n",
    "                   'llm_probability', 'llm_response', 'abs_error', 'rel_error', 'target_tw', 'n', 'seed', 'variant_index',\n",
    "                   'alpha', 'determinism', 'arity', 'query_index', \n",
    "                   'achieved_tw', 'num_nodes'}\n",
    "    \n",
    "    # Get all columns that are BN or query properties\n",
    "    property_cols = [col for col in full_df.columns if col not in exclude_cols]\n",
    "    \n",
    "    # Calculate number of subplots needed\n",
    "    n_props = len(property_cols)\n",
    "    n_cols = min(4, n_props)  # Max 4 columns\n",
    "    n_rows = (n_props + n_cols - 1) // n_cols  # Ceiling division\n",
    "    \n",
    "    # Create figure with subplots for all properties\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(4*n_cols, 3*n_rows))\n",
    "    fig.suptitle('Absolute Error by BN and Query Properties', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Flatten axes for easier indexing\n",
    "    if n_props == 1:\n",
    "        axes = [axes]\n",
    "    elif n_rows == 1:\n",
    "        axes = axes.flatten()\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    # Create boxplots for each property\n",
    "    for i, prop in enumerate(property_cols):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Get unique values for this property\n",
    "        unique_vals = sorted(plot_df[prop].unique())\n",
    "        \n",
    "        # Create boxplot data\n",
    "        box_data = []\n",
    "        labels = []\n",
    "        \n",
    "        for val in unique_vals:\n",
    "            subset = plot_df[plot_df[prop] == val]['abs_error']\n",
    "            if len(subset) > 0:  # Only include if there's data\n",
    "                box_data.append(subset.values)\n",
    "                labels.append(str(val))\n",
    "        \n",
    "        if box_data:  # Only plot if we have data\n",
    "            ax.boxplot(box_data, labels=labels)\n",
    "            ax.set_title(f'Absolute Error by {prop.replace(\"_\", \" \").title()}')\n",
    "            ax.set_xlabel(prop.replace(\"_\", \" \").title())\n",
    "            ax.set_ylabel('Absolute Error')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Rotate x-axis labels if there are many unique values\n",
    "            if len(labels) > 5:\n",
    "                ax.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(n_props, len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics for each property\n",
    "    print(\"\\nSummary Statistics by Property:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for prop in property_cols:\n",
    "        print(f\"\\n{prop.upper()}:\")\n",
    "        prop_stats = plot_df.groupby(prop)['abs_error'].agg(['count', 'mean', 'std', 'min', 'max'])\n",
    "        print(prop_stats.round(6))\n",
    "        \n",
    "else:\n",
    "    print(\"No successful LLM responses with exact inference results available for plotting.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function re-defined successfully!\n"
     ]
    }
   ],
   "source": [
    "# Re-define the function to ensure it's properly loaded with the verbose parameter\n",
    "def compute_query_complexity(full_df, all_bayesian_networks, row_index, verbose=True):\n",
    "    \"\"\"\n",
    "    Compute the complexity metrics for a specific query from a row in full_df.\n",
    "    Now properly takes the query into account by only eliminating non-query variables.\n",
    "    \n",
    "    Parameters:\n",
    "    - full_df: DataFrame containing query information\n",
    "    - all_bayesian_networks: List of BN dictionaries with 'bn' and 'meta' keys\n",
    "    - row_index: Index of the row in full_df to analyze\n",
    "    - verbose: If True, print detailed progress information\n",
    "    \n",
    "    Returns:\n",
    "    - dict: Complexity metrics including induced width, total cost, max factor size, etc.\n",
    "    \"\"\"\n",
    "    from pgmpy.inference.EliminationOrder import WeightedMinFill\n",
    "    from pgmpy.inference import VariableElimination\n",
    "    from bn_query_sweep import _parse_field\n",
    "    \n",
    "    # Get the row data\n",
    "    row = full_df.iloc[row_index]\n",
    "    bn_index = int(row['bn_index'])\n",
    "    \n",
    "    # Get the Bayesian network\n",
    "    bn = all_bayesian_networks[bn_index]['bn']\n",
    "    \n",
    "    # Parse query information\n",
    "    query_vars = _parse_field(row['query_vars']) or []\n",
    "    query_states = _parse_field(row['query_states']) or []\n",
    "    evidence = _parse_field(row['evidence']) or {}\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Computing complexity for query: P({query_vars}={query_states} | {evidence})\")\n",
    "        print(f\"BN: {bn_index}, Query: {int(row['query_index'])}\")\n",
    "    \n",
    "    # Ensure the model is valid\n",
    "    bn.check_model()\n",
    "    \n",
    "    # Get cardinalities\n",
    "    card = bn.get_cardinality()\n",
    "    if verbose:\n",
    "        print(f\"Variable cardinalities: {card}\")\n",
    "    \n",
    "    # QUERY-SPECIFIC COMPLEXITY COMPUTATION\n",
    "    # Identify which variables need to be eliminated vs kept for the query\n",
    "    all_vars = set(bn.nodes())\n",
    "    query_vars_set = set(query_vars)\n",
    "    evidence_vars_set = set(evidence.keys()) if evidence else set()\n",
    "    \n",
    "    # Variables that must be kept until the end (query variables)\n",
    "    keep_vars = query_vars_set\n",
    "    \n",
    "    # Variables that can be eliminated (all others)\n",
    "    eliminate_vars = all_vars - keep_vars\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Variables to keep (query): {sorted(keep_vars)}\")\n",
    "        print(f\"Variables to eliminate: {sorted(eliminate_vars)}\")\n",
    "        print(f\"Evidence variables: {sorted(evidence_vars_set)}\")\n",
    "    \n",
    "    # Handle evidence by reducing cardinalities\n",
    "    # Evidence variables are instantiated, so they don't contribute to factor sizes\n",
    "    effective_card = card.copy()\n",
    "    for evar in evidence_vars_set:\n",
    "        effective_card[evar] = 1  # Evidence variables are fixed, so cardinality = 1\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Effective cardinalities (after evidence): {dict(effective_card)}\")\n",
    "    \n",
    "    # Create elimination orderer and get optimal order for variables to eliminate\n",
    "    if eliminate_vars:\n",
    "        orderer = WeightedMinFill(bn)\n",
    "        elim_order = orderer.get_elimination_order(nodes=list(eliminate_vars))\n",
    "    else:\n",
    "        elim_order = []  # No variables to eliminate\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Elimination order (variables to eliminate): {elim_order}\")\n",
    "        if elim_order:\n",
    "            complete_elim_order = elim_order + list(keep_vars)\n",
    "            print(f\"Complete elimination order: {complete_elim_order}\")\n",
    "    \n",
    "    # Calculate induced width for the elimination order\n",
    "    if elim_order:\n",
    "        # For induced width calculation, we need to create a complete elimination order\n",
    "        # that includes all variables, with query variables at the end\n",
    "        complete_elim_order = elim_order + list(keep_vars)\n",
    "        ve = VariableElimination(bn)\n",
    "        induced_width = ve.induced_width(complete_elim_order)\n",
    "    else:\n",
    "        complete_elim_order = list(keep_vars)  # Only query variables\n",
    "        induced_width = 0  # No elimination needed\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Induced width: {induced_width}\")\n",
    "    \n",
    "    # Simulate variable elimination to compute cost metrics\n",
    "    cost = 0\n",
    "    max_factor_size = 0\n",
    "    moral = bn.to_markov_model()  # moralized undirected graph\n",
    "    \n",
    "    # Track factor sizes for each elimination step\n",
    "    factor_sizes = []\n",
    "    \n",
    "    for step, x in enumerate(elim_order):\n",
    "        nbrs = list(moral.neighbors(x))\n",
    "        \n",
    "        # Size of the intermediate factor created when eliminating x\n",
    "        # Use effective cardinalities (evidence variables have cardinality 1)\n",
    "        size = 1\n",
    "        for v in nbrs + [x]:\n",
    "            size *= effective_card[v]\n",
    "        \n",
    "        cost += size\n",
    "        max_factor_size = max(max_factor_size, size)\n",
    "        factor_sizes.append(size)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Step {step+1}: Eliminating {x}, neighbors: {nbrs}, factor size: {size}\")\n",
    "        \n",
    "        # Connect neighbors (fill-in) and remove x\n",
    "        for i in range(len(nbrs)):\n",
    "            for j in range(i+1, len(nbrs)):\n",
    "                moral.add_edge(nbrs[i], nbrs[j])\n",
    "        moral.remove_node(x)\n",
    "    \n",
    "    # Calculate final factor size (the remaining query variables)\n",
    "    if keep_vars:\n",
    "        # The final factor contains all remaining query variables\n",
    "        final_factor_size = 1\n",
    "        for v in keep_vars:\n",
    "            final_factor_size *= effective_card[v]\n",
    "        cost += final_factor_size\n",
    "        max_factor_size = max(max_factor_size, final_factor_size)\n",
    "        if verbose:\n",
    "            print(f\"Final factor (query variables): {sorted(keep_vars)}, size: {final_factor_size}\")\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    num_vars = len(bn.nodes())\n",
    "    num_edges = bn.number_of_edges()\n",
    "    \n",
    "    # Query-specific metrics\n",
    "    num_query_vars = len(query_vars)\n",
    "    num_evidence_vars = len(evidence) if evidence else 0\n",
    "    num_eliminated_vars = len(elim_order)\n",
    "    \n",
    "    # Complexity metrics\n",
    "    complexity_metrics = {\n",
    "        'row_index': row_index,\n",
    "        'bn_index': bn_index,\n",
    "        'query_index': int(row['query_index']),\n",
    "        'query_vars': query_vars,\n",
    "        'query_states': query_states,\n",
    "        'evidence': evidence,\n",
    "        'num_vars': num_vars,\n",
    "        'num_edges': num_edges,\n",
    "        'num_query_vars': num_query_vars,\n",
    "        'num_evidence_vars': num_evidence_vars,\n",
    "        'num_eliminated_vars': num_eliminated_vars,\n",
    "        'elimination_order': elim_order,\n",
    "        'complete_elimination_order': complete_elim_order,\n",
    "        'induced_width': induced_width,\n",
    "        'total_cost': cost,\n",
    "        'max_factor_size': max_factor_size,\n",
    "        'avg_factor_size': cost / len(elim_order) if elim_order else 0,\n",
    "        'factor_sizes': factor_sizes,\n",
    "        'log_total_cost': np.log2(cost) if cost > 0 else 0,\n",
    "        'log_max_factor_size': np.log2(max_factor_size) if max_factor_size > 0 else 0,\n",
    "        'keep_vars': sorted(keep_vars),\n",
    "        'eliminate_vars': sorted(eliminate_vars),\n",
    "    }\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nQuery-Specific Complexity Summary:\")\n",
    "        print(f\"  Variables eliminated: {num_eliminated_vars}/{num_vars}\")\n",
    "        print(f\"  Query variables kept: {sorted(keep_vars)}\")\n",
    "        print(f\"  Induced width: {induced_width}\")\n",
    "        print(f\"  Total factor work: {cost:,}\")\n",
    "        print(f\"  Max intermediate factor size: {max_factor_size:,}\")\n",
    "        print(f\"  Average factor size: {cost / len(elim_order) if elim_order else 0:.1f}\")\n",
    "        print(f\"  Log2(total cost): {np.log2(cost):.2f}\")\n",
    "        print(f\"  Log2(max factor size): {np.log2(max_factor_size):.2f}\")\n",
    "    \n",
    "    return complexity_metrics\n",
    "\n",
    "print(\"Function re-defined successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TESTING QUERY-SPECIFIC COMPLEXITY COMPUTATION FOR ROW 0\n",
      "================================================================================\n",
      "Computing complexity for query: P(['K_kx9xcv']=['s2'] | {'O_cguird': 's1'})\n",
      "BN: 0, Query: 0\n",
      "Variable cardinalities: defaultdict(<class 'int'>, {'G_2ohusw': np.int64(2), 'O_gh8nq9': np.int64(3), 'K_kx9xcv': np.int64(3), 'U_bx3bu6': np.int64(3), 'Y_nizbt1': np.int64(2), 'Z_7j9n4o': np.int64(2), 'L_vy01po': np.int64(2), 'O_cguird': np.int64(3), 'R_dbfd2r': np.int64(2)})\n",
      "Variables to keep (query): ['K_kx9xcv']\n",
      "Variables to eliminate: ['G_2ohusw', 'L_vy01po', 'O_cguird', 'O_gh8nq9', 'R_dbfd2r', 'U_bx3bu6', 'Y_nizbt1', 'Z_7j9n4o']\n",
      "Evidence variables: ['O_cguird']\n",
      "Effective cardinalities (after evidence): {'G_2ohusw': np.int64(2), 'O_gh8nq9': np.int64(3), 'K_kx9xcv': np.int64(3), 'U_bx3bu6': np.int64(3), 'Y_nizbt1': np.int64(2), 'Z_7j9n4o': np.int64(2), 'L_vy01po': np.int64(2), 'O_cguird': 1, 'R_dbfd2r': np.int64(2)}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8191ca69c7a74d759407e6e790935b02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elimination order (variables to eliminate): ['Y_nizbt1', 'O_gh8nq9', 'O_cguird', 'G_2ohusw', 'Z_7j9n4o', 'U_bx3bu6', 'L_vy01po', 'R_dbfd2r']\n",
      "Complete elimination order: ['Y_nizbt1', 'O_gh8nq9', 'O_cguird', 'G_2ohusw', 'Z_7j9n4o', 'U_bx3bu6', 'L_vy01po', 'R_dbfd2r', 'K_kx9xcv']\n",
      "Induced width: 3\n",
      "Step 1: Eliminating Y_nizbt1, neighbors: ['K_kx9xcv'], factor size: 6\n",
      "Step 2: Eliminating O_gh8nq9, neighbors: ['G_2ohusw'], factor size: 6\n",
      "Step 3: Eliminating O_cguird, neighbors: ['L_vy01po', 'G_2ohusw'], factor size: 4\n",
      "Step 4: Eliminating G_2ohusw, neighbors: ['L_vy01po', 'K_kx9xcv'], factor size: 12\n",
      "Step 5: Eliminating Z_7j9n4o, neighbors: ['U_bx3bu6', 'K_kx9xcv'], factor size: 18\n",
      "Step 6: Eliminating U_bx3bu6, neighbors: ['L_vy01po', 'R_dbfd2r', 'K_kx9xcv'], factor size: 36\n",
      "Step 7: Eliminating L_vy01po, neighbors: ['R_dbfd2r', 'K_kx9xcv'], factor size: 12\n",
      "Step 8: Eliminating R_dbfd2r, neighbors: ['K_kx9xcv'], factor size: 6\n",
      "Final factor (query variables): ['K_kx9xcv'], size: 3\n",
      "\n",
      "Query-Specific Complexity Summary:\n",
      "  Variables eliminated: 8/9\n",
      "  Query variables kept: ['K_kx9xcv']\n",
      "  Induced width: 3\n",
      "  Total factor work: 103\n",
      "  Max intermediate factor size: 36\n",
      "  Average factor size: 12.9\n",
      "  Log2(total cost): 6.69\n",
      "  Log2(max factor size): 5.17\n",
      "\n",
      "================================================================================\n",
      "DETAILED COMPLEXITY METRICS:\n",
      "================================================================================\n",
      "row_index                : 0\n",
      "bn_index                 : 0\n",
      "query_index              : 0\n",
      "query_vars               : ['K_kx9xcv']\n",
      "query_states             : ['s2']\n",
      "evidence                 : {'O_cguird': 's1'}\n",
      "num_vars                 : 9\n",
      "num_edges                : 12\n",
      "num_query_vars           : 1\n",
      "num_evidence_vars        : 1\n",
      "num_eliminated_vars      : 8\n",
      "elimination_order        : ['Y_nizbt1', 'O_gh8nq9', 'O_cguird', 'G_2ohusw', 'Z_7j9n4o']... (showing first 5 of 8)\n",
      "complete_elimination_order: ['Y_nizbt1', 'O_gh8nq9', 'O_cguird', 'G_2ohusw', 'Z_7j9n4o']... (showing first 5 of 9)\n",
      "induced_width            : 3\n",
      "total_cost               : 103\n",
      "max_factor_size          : 36\n",
      "avg_factor_size          : 12.875\n",
      "factor_sizes             : [np.int64(6), np.int64(6), np.int64(4), np.int64(12), np.int64(18)]... (showing first 5 of 8)\n",
      "log_total_cost           : 6.6865005271832185\n",
      "log_max_factor_size      : 5.169925001442312\n",
      "keep_vars                : ['K_kx9xcv']\n",
      "eliminate_vars           : ['G_2ohusw', 'L_vy01po', 'O_cguird', 'O_gh8nq9', 'R_dbfd2r', 'U_bx3bu6', 'Y_nizbt1', 'Z_7j9n4o']\n",
      "\n",
      "================================================================================\n",
      "FACTOR SIZE PROGRESSION:\n",
      "================================================================================\n",
      "Step  1:        6 entries\n",
      "Step  2:        6 entries\n",
      "Step  3:        4 entries\n",
      "Step  4:       12 entries\n",
      "Step  5:       18 entries\n",
      "Step  6:       36 entries\n",
      "Step  7:       12 entries\n",
      "Step  8:        6 entries\n",
      "\n",
      "================================================================================\n",
      "QUERY-SPECIFIC ANALYSIS:\n",
      "================================================================================\n",
      "Query variables kept: ['K_kx9xcv']\n",
      "Variables eliminated: ['G_2ohusw', 'L_vy01po', 'O_cguird', 'O_gh8nq9', 'R_dbfd2r', 'U_bx3bu6', 'Y_nizbt1', 'Z_7j9n4o']\n",
      "Variables eliminated: 8/9 (88.9%)\n"
     ]
    }
   ],
   "source": [
    "# Test the updated query-specific complexity computation function\n",
    "# Choose a row index from full_df to analyze\n",
    "test_row_index = 0  # Change this to any valid row index\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"TESTING QUERY-SPECIFIC COMPLEXITY COMPUTATION FOR ROW {test_row_index}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Compute complexity for the selected row\n",
    "complexity_result = compute_query_complexity(full_df, all_bayesian_networks, test_row_index, verbose=True)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DETAILED COMPLEXITY METRICS:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Display the results in a nice format\n",
    "for key, value in complexity_result.items():\n",
    "    if key not in ['elimination_order', 'complete_elimination_order', 'factor_sizes', 'query_vars', 'query_states', 'evidence', 'keep_vars', 'eliminate_vars']:\n",
    "        print(f\"{key:25}: {value}\")\n",
    "    elif key in ['elimination_order', 'complete_elimination_order']:\n",
    "        print(f\"{key:25}: {value[:5]}... (showing first 5 of {len(value)})\")\n",
    "    elif key == 'factor_sizes':\n",
    "        print(f\"{key:25}: {value[:5]}... (showing first 5 of {len(value)})\")\n",
    "    else:\n",
    "        print(f\"{key:25}: {value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FACTOR SIZE PROGRESSION:\")\n",
    "print(\"=\" * 80)\n",
    "for i, size in enumerate(complexity_result['factor_sizes']):\n",
    "    print(f\"Step {i+1:2d}: {size:8,} entries\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"QUERY-SPECIFIC ANALYSIS:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Query variables kept: {complexity_result['keep_vars']}\")\n",
    "print(f\"Variables eliminated: {complexity_result['eliminate_vars']}\")\n",
    "print(f\"Variables eliminated: {complexity_result['num_eliminated_vars']}/{complexity_result['num_vars']} ({complexity_result['num_eliminated_vars']/complexity_result['num_vars']*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing query 1/48...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec586e6ce77c4631aea25d7e5b5eebdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing query 2/48...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ea25822705c4ddea16b6982477d1dff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing query 3/48...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "397065d5d2264116bd80c1b36b93077f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing query 4/48...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f6b4a64c779477d94a57338c8acd8a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing query 5/48...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "486d7fb8fba949c38900e9e233c5a6fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing query 6/48...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da6fd249f32e48dfacd25e7c5fef58c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing query 7/48...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "039b22428cea46aeb8bf56c313029b72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing query 8/48...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a32e58a3e30414485840cfe1b2adbb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing query 9/48...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8819a7bb7447478db10a2113881ec7c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing query 10/48...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f50540478cb84b38a5d66339f427e286",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing query 11/48...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "defe308544f54d1ca5fed1bcb15106fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing query 12/48...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39aafef91e08488bbfb1103ec2621caa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing query 13/48...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "702cad684121443bb6294efa8b1d50e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing query 14/48...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98a781a5170b469f9ec44155bce83ac2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing query 15/48...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4e426045ec74d9ea5a4a73e976bfc53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing query 16/48...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0647c31a59434c74b6957794c6e5129f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing query 17/48...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d947a46346b4a30857c1db6b4bee972",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing query 18/48...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ac37fa0634a40f0acc2eadba5343706",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing query 19/48...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9169783a16a4d9589015145b62a120a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing query 20/48...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93c0393acb5e4f1889bab85fdd969106",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing query 21/48...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ffcff6e2302489896293d0601087be0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing query 22/48...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00ecf6ac413b4a6882fe44ecbfb14254",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing query 23/48...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd0fa10fb20a4134aed7e832d513240a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing query 24/48...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6182322956b04a1085d9ec04daa600ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing query 25/48...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "817344860f0e45bdbf0b7d9c2e082c2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing query 26/48...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85ab087579ac483eb01460e9f050eeb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing query 27/48...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75f0b58ade83498c95b2799d3ad92d80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing query 28/48...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2ec2eae7e784778815ca15b0f19d62c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing query 29/48...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91876282339a4869a80b5881021c8708",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing query 30/48...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57cc39d9e5fe4e06886ca5ea6066893e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing query 31/48...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d728ccbd159341498299ef089b7b5227",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing query 32/48...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acba289e6fae49a7860bb2e22e26bbf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing query 33/48...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed1d80750e364c26902ad74eaa1dabf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing query 34/48...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc3c2c6b04494d0b8276b8ac61f32949",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing query 35/48...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb24cad28e6c4daea91a16f21b3f5164",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing query 36/48...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cd3ee9226704c968045031af2f2efaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing query 37/48...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd87d0bb41904322ab989166fd405513",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing query 38/48...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba774e61d24f47f0942fb390b9be69fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing query 39/48...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c16d1ab8566e47bfaffd531072c9a311",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing query 40/48...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47b352813ff845d4ae18b80009a3f102",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing query 41/48...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91a3485c75bf4a2e8c6e469c20f7e5f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing query 42/48...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95513454cdc545f89b2106a2cf4c814a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing query 43/48...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53c75812d06348bf92e7bffbcb2605bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing query 44/48...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5618d3c8d2fa4f5786f37c80e6f66489",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing query 45/48...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c856c800b72c4b788b2704ed57b72008",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing query 46/48...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5446bb4c1347431d9fba8580f9ca31d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing query 47/48...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a295344d9fe4d5082cda3ffcc013d95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing query 48/48...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9440530a5a9044bbb020f88d583df739",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computed complexity for 48 queries\n",
      "Successful computations: 48\n",
      "Failed computations: 0\n",
      "\n",
      "Complexity DataFrame:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_index</th>\n",
       "      <th>bn_index</th>\n",
       "      <th>query_index</th>\n",
       "      <th>query_vars</th>\n",
       "      <th>query_states</th>\n",
       "      <th>evidence</th>\n",
       "      <th>num_vars</th>\n",
       "      <th>num_edges</th>\n",
       "      <th>num_query_vars</th>\n",
       "      <th>num_evidence_vars</th>\n",
       "      <th>...</th>\n",
       "      <th>complete_elimination_order</th>\n",
       "      <th>induced_width</th>\n",
       "      <th>total_cost</th>\n",
       "      <th>max_factor_size</th>\n",
       "      <th>avg_factor_size</th>\n",
       "      <th>factor_sizes</th>\n",
       "      <th>log_total_cost</th>\n",
       "      <th>log_max_factor_size</th>\n",
       "      <th>keep_vars</th>\n",
       "      <th>eliminate_vars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[K_kx9xcv]</td>\n",
       "      <td>[s2]</td>\n",
       "      <td>{'O_cguird': 's1'}</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>[Y_nizbt1, O_gh8nq9, O_cguird, G_2ohusw, Z_7j9...</td>\n",
       "      <td>3</td>\n",
       "      <td>103</td>\n",
       "      <td>36</td>\n",
       "      <td>12.875000</td>\n",
       "      <td>[6, 6, 4, 12, 18, 36, 12, 6]</td>\n",
       "      <td>6.686501</td>\n",
       "      <td>5.169925</td>\n",
       "      <td>[K_kx9xcv]</td>\n",
       "      <td>[G_2ohusw, L_vy01po, O_cguird, O_gh8nq9, R_dbf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[Z_7j9n4o]</td>\n",
       "      <td>[s1]</td>\n",
       "      <td>{}</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>[Y_nizbt1, O_gh8nq9, O_cguird, G_2ohusw, L_vy0...</td>\n",
       "      <td>3</td>\n",
       "      <td>116</td>\n",
       "      <td>36</td>\n",
       "      <td>14.500000</td>\n",
       "      <td>[6, 6, 12, 12, 36, 18, 18, 6]</td>\n",
       "      <td>6.857981</td>\n",
       "      <td>5.169925</td>\n",
       "      <td>[Z_7j9n4o]</td>\n",
       "      <td>[G_2ohusw, K_kx9xcv, L_vy01po, O_cguird, O_gh8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>[Z_7j9n4o]</td>\n",
       "      <td>[s0]</td>\n",
       "      <td>{}</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>[Y_nizbt1, O_gh8nq9, O_cguird, G_2ohusw, L_vy0...</td>\n",
       "      <td>3</td>\n",
       "      <td>116</td>\n",
       "      <td>36</td>\n",
       "      <td>14.500000</td>\n",
       "      <td>[6, 6, 12, 12, 36, 18, 18, 6]</td>\n",
       "      <td>6.857981</td>\n",
       "      <td>5.169925</td>\n",
       "      <td>[Z_7j9n4o]</td>\n",
       "      <td>[G_2ohusw, K_kx9xcv, L_vy01po, O_cguird, O_gh8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>[O_cguird]</td>\n",
       "      <td>[s2]</td>\n",
       "      <td>{'R_dbfd2r': 's1', 'K_kx9xcv': 's2'}</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>[Y_nizbt1, O_gh8nq9, Z_7j9n4o, U_bx3bu6, R_dbf...</td>\n",
       "      <td>3</td>\n",
       "      <td>47</td>\n",
       "      <td>12</td>\n",
       "      <td>5.875000</td>\n",
       "      <td>[2, 6, 6, 6, 2, 4, 12, 6]</td>\n",
       "      <td>5.554589</td>\n",
       "      <td>3.584963</td>\n",
       "      <td>[O_cguird]</td>\n",
       "      <td>[G_2ohusw, K_kx9xcv, L_vy01po, O_gh8nq9, R_dbf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>[O_gh8nq9, O_cguird]</td>\n",
       "      <td>[s2, s0]</td>\n",
       "      <td>{}</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>[Y_nizbt1, Z_7j9n4o, U_bx3bu6, R_dbfd2r, K_kx9...</td>\n",
       "      <td>3</td>\n",
       "      <td>123</td>\n",
       "      <td>36</td>\n",
       "      <td>17.571429</td>\n",
       "      <td>[6, 18, 36, 12, 12, 12, 18]</td>\n",
       "      <td>6.942515</td>\n",
       "      <td>5.169925</td>\n",
       "      <td>[O_cguird, O_gh8nq9]</td>\n",
       "      <td>[G_2ohusw, K_kx9xcv, L_vy01po, R_dbfd2r, U_bx3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_index  bn_index  query_index            query_vars query_states  \\\n",
       "0          0         0            0            [K_kx9xcv]         [s2]   \n",
       "1          1         0            1            [Z_7j9n4o]         [s1]   \n",
       "2          2         0            2            [Z_7j9n4o]         [s0]   \n",
       "3          3         0            3            [O_cguird]         [s2]   \n",
       "4          4         0            4  [O_gh8nq9, O_cguird]     [s2, s0]   \n",
       "\n",
       "                               evidence  num_vars  num_edges  num_query_vars  \\\n",
       "0                    {'O_cguird': 's1'}         9         12               1   \n",
       "1                                    {}         9         12               1   \n",
       "2                                    {}         9         12               1   \n",
       "3  {'R_dbfd2r': 's1', 'K_kx9xcv': 's2'}         9         12               1   \n",
       "4                                    {}         9         12               2   \n",
       "\n",
       "   num_evidence_vars  ...                         complete_elimination_order  \\\n",
       "0                  1  ...  [Y_nizbt1, O_gh8nq9, O_cguird, G_2ohusw, Z_7j9...   \n",
       "1                  0  ...  [Y_nizbt1, O_gh8nq9, O_cguird, G_2ohusw, L_vy0...   \n",
       "2                  0  ...  [Y_nizbt1, O_gh8nq9, O_cguird, G_2ohusw, L_vy0...   \n",
       "3                  2  ...  [Y_nizbt1, O_gh8nq9, Z_7j9n4o, U_bx3bu6, R_dbf...   \n",
       "4                  0  ...  [Y_nizbt1, Z_7j9n4o, U_bx3bu6, R_dbfd2r, K_kx9...   \n",
       "\n",
       "  induced_width total_cost  max_factor_size  avg_factor_size  \\\n",
       "0             3        103               36        12.875000   \n",
       "1             3        116               36        14.500000   \n",
       "2             3        116               36        14.500000   \n",
       "3             3         47               12         5.875000   \n",
       "4             3        123               36        17.571429   \n",
       "\n",
       "                    factor_sizes  log_total_cost log_max_factor_size  \\\n",
       "0   [6, 6, 4, 12, 18, 36, 12, 6]        6.686501            5.169925   \n",
       "1  [6, 6, 12, 12, 36, 18, 18, 6]        6.857981            5.169925   \n",
       "2  [6, 6, 12, 12, 36, 18, 18, 6]        6.857981            5.169925   \n",
       "3      [2, 6, 6, 6, 2, 4, 12, 6]        5.554589            3.584963   \n",
       "4    [6, 18, 36, 12, 12, 12, 18]        6.942515            5.169925   \n",
       "\n",
       "              keep_vars                                     eliminate_vars  \n",
       "0            [K_kx9xcv]  [G_2ohusw, L_vy01po, O_cguird, O_gh8nq9, R_dbf...  \n",
       "1            [Z_7j9n4o]  [G_2ohusw, K_kx9xcv, L_vy01po, O_cguird, O_gh8...  \n",
       "2            [Z_7j9n4o]  [G_2ohusw, K_kx9xcv, L_vy01po, O_cguird, O_gh8...  \n",
       "3            [O_cguird]  [G_2ohusw, K_kx9xcv, L_vy01po, O_gh8nq9, R_dbf...  \n",
       "4  [O_cguird, O_gh8nq9]  [G_2ohusw, K_kx9xcv, L_vy01po, R_dbfd2r, U_bx3...  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def compute_all_query_complexities(full_df, all_bayesian_networks, verbose=False):\n",
    "    \"\"\"\n",
    "    Compute complexity metrics for all queries in full_df.\n",
    "    \n",
    "    Parameters:\n",
    "    - full_df: DataFrame containing query information\n",
    "    - all_bayesian_networks: List of BN dictionaries with 'bn' and 'meta' keys\n",
    "    - verbose: If True, print progress information\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame with complexity metrics for each query\n",
    "    \"\"\"\n",
    "    complexity_results = []\n",
    "    \n",
    "    for idx in range(len(full_df)):\n",
    "        if verbose:\n",
    "            print(f\"Processing query {idx+1}/{len(full_df)}...\")\n",
    "        \n",
    "        try:\n",
    "            result = compute_query_complexity(full_df, all_bayesian_networks, idx, verbose=False)\n",
    "            complexity_results.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing row {idx}: {e}\")\n",
    "            # Add a row with error information\n",
    "            complexity_results.append({\n",
    "                'row_index': idx,\n",
    "                'error': str(e),\n",
    "                'induced_width': None,\n",
    "                'total_cost': None,\n",
    "                'max_factor_size': None,\n",
    "            })\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    complexity_df = pd.DataFrame(complexity_results)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nComputed complexity for {len(complexity_results)} queries\")\n",
    "        if 'error' in complexity_df.columns:\n",
    "            successful = len(complexity_df[complexity_df['error'].isna()])\n",
    "            failed = len(complexity_df[complexity_df['error'].notna()])\n",
    "        else:\n",
    "            successful = len(complexity_df)\n",
    "            failed = 0\n",
    "        print(f\"Successful computations: {successful}\")\n",
    "        print(f\"Failed computations: {failed}\")\n",
    "    \n",
    "    return complexity_df\n",
    "\n",
    "# Example usage for a subset of queries (uncomment to run)\n",
    "complexity_df = compute_all_query_complexities(full_df, all_bayesian_networks, verbose=True)\n",
    "print(\"\\nComplexity DataFrame:\")\n",
    "display(complexity_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM calls are disabled. Skipping single row inspection with LLM.\n",
      "Query: P([np.str_('R_dbfd2r'), np.str_('G_2ohusw')]=['s0', 's1'] | {np.str_('Y_nizbt1'): 's1'})\n",
      "Exact probability: 0.2072200625966507\n"
     ]
    }
   ],
   "source": [
    "# Single row inspection with LLM toggle\n",
    "from bn_query_sweep import inspect_row_and_call_llm, call_llm_for_query\n",
    "from pathlib import Path\n",
    "\n",
    "# Choose a row index from full_df\n",
    "#row_index = 330\n",
    "row_index = 8\n",
    "\n",
    "if ENABLE_LLM_CALLS:\n",
    "    result = inspect_row_and_call_llm(\n",
    "        full_df=full_df,\n",
    "        all_bayesian_networks=all_bayesian_networks,\n",
    "        row_index=row_index,\n",
    "        openai_client=client,\n",
    "        model=MODEL,\n",
    "        prompts_path=prompt_path,\n",
    "        draw_kwargs={\"figsize\": (6, 4)},\n",
    "    )\n",
    "    print(result)\n",
    "else:\n",
    "    print(\"LLM calls are disabled. Skipping single row inspection with LLM.\")\n",
    "    # Still show the query details without LLM call\n",
    "    row = full_df.iloc[row_index]\n",
    "    print(f\"Query: P({row['query_vars']}={row['query_states']} | {row['evidence']})\")\n",
    "    print(f\"Exact probability: {row['probability']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
